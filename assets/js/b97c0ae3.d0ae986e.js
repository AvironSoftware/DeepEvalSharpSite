"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[79],{1497:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"metrics/SummarizationMetric","title":"Summarization Metric","description":"The Summarization Metric leverages an LLM as a judge to assess if your model\u2019s ActualOutput is both factually accurate and sufficiently comprehensive when compared to the original InitialInput. It generates two sub-scores\u2014a contradiction-and-fabrication\u2013detecting alignment score and a key-information\u2013measuring coverage score\u2014takes the lower of the two as the final result, and supplies a human-readable explanation for its judgment, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/SummarizationMetric.md","sourceDirName":"metrics","slug":"/metrics/SummarizationMetric","permalink":"/docs/metrics/SummarizationMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Prompt Alignment Metric","permalink":"/docs/metrics/PromptAlignmentMetric"},"next":{"title":"Task Completion Metric","permalink":"/docs/metrics/TaskCompletionMetric"}}');var i=r(4848),s=r(8453);const a={},o="Summarization Metric",l={},c=[{value:"When you should use Summarization Metric",id:"when-you-should-use-summarization-metric",level:4},{value:"When you SHOULDN&#39;T use Summarization Metric",id:"when-you-shouldnt-use-summarization-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"summarization-metric",children:"Summarization Metric"})}),"\n",(0,i.jsxs)(t.p,{children:["The Summarization Metric leverages an LLM as a judge to assess if your model\u2019s ",(0,i.jsx)(t.code,{children:"ActualOutput"})," is both factually accurate and sufficiently comprehensive when compared to the original ",(0,i.jsx)(t.code,{children:"InitialInput"}),". It generates two sub-scores\u2014a contradiction-and-fabrication\u2013detecting alignment score and a key-information\u2013measuring coverage score\u2014takes the lower of the two as the final result, and supplies a human-readable explanation for its judgment, making it a self-explaining LLM-Eval tool."]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-should-use-summarization-metric",children:"When you should use Summarization Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Assessing Summary Fidelity"})," \u2013 Ensure summaries accurately reflect source text without fabrications or contradictions."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Evaluating Information Coverage"})," \u2013 Verify that the summary includes all necessary details from the original content."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Benchmarking Summarization Quality"})," \u2013 Compare different models or configurations on their ability to generate faithful, comprehensive summaries."]}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-shouldnt-use-summarization-metric",children:"When you SHOULDN'T use Summarization Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Fact-Checking with Explicit References"})," \u2013 For context-backed validation, metrics like Hallucination or Faithfulness are more appropriate."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Creative or Abstracted Summaries"})," \u2013 If you require imaginative rewrites rather than faithful representation."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"High-Throughput/Cacheable Requirements"})," \u2013 As a non-cacheable metric requiring multiple LLM calls, it may not suit large-scale pipelines."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsxs)(t.p,{children:["The Summarization Metric requires ",(0,i.jsx)(t.code,{children:"InitialInput"})," and ",(0,i.jsx)(t.code,{children:"ActualOutput"})," to function. You can instantiate an Summarization metric with optional parameters to customize its behavior. If AssessmentQuestions are provided, NumQuestions will be ignored as NumQuestions is used for asking the LLM to create the AssessmentQuestions for when they're not provided."]}),"\n",(0,i.jsx)(t.p,{children:"Add Summarization Metric to your evaluator:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Method"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddSummarization(int numQuestions = 5, int? truthsExtractionLimit = null, bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,i.jsx)(t.td,{children:"Creates the Summarization metric and adds it to the evaluator."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddSummarization(List<string> assessmentQuestions, int? truthsExtractionLimit = null, bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,i.jsx)(t.td,{children:"Creates the Summarization metric and adds it to the evaluator."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddSummarization(SummarizationMetricConfiguration config)"})}),(0,i.jsx)(t.td,{children:"Creates the Summarization metric and adds it to the evaluator."})]})]})]}),"\n",(0,i.jsx)(t.p,{children:"Here's an example of how to use Summarization metric:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "A study on sleep patterns found that adults who sleep 7-8 hours a night perform better on cognitive tests and report better mental health. The study analyzed 10,000 adults across the US.",\r\n        LLMOutput   = "A US study of 10,000 adults found that sleeping 7-8 hours improves mental health and cognitive performance."\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        InitialInput    = c.UserInput,\r\n        ActualOutput    = c.LLMOutput\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddSummarization(assessmentQuestions: [\r\n                "Does the summary mention that the study included 10,000 adults?",\r\n                "Does the summary correctly identify 7-8 hours of sleep as the optimal range?",\r\n                "Does the summary mention improved cognitive performance as a result?",\r\n                "Does the summary include improved mental health as a finding?",\r\n                "Does the summary indicate that the study was conducted in the US?"\r\n            ]);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,i.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"InitialInput"})}),(0,i.jsx)(t.td,{children:"A string That represents the initial input is the user interaction with the LLM."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"ActualOutput"})}),(0,i.jsx)(t.td,{children:"A string That represents the actual output of the test case from the LLM."})]})]})]}),"\n",(0,i.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"NumQuestions"})}),(0,i.jsx)(t.td,{children:"Integer representing number of assessment questions to generate when none are provided. Defaults to 5."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"TruthsExtractionLimit"})}),(0,i.jsx)(t.td,{children:"Integer representing the maximum number of factual claims to extract from the original text (input); null lets the LLM choose."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AssessmentQuestions"})}),(0,i.jsx)(t.td,{children:"List strings that represent of close-ended questions to assess summary quality that can be answered with yes or no."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"Threshold"})}),(0,i.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"IncludeReason"})}),(0,i.jsxs)(t.td,{children:["A boolean that, when set to ",(0,i.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,i.jsx)(t.code,{children:"True"}),"."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"StrictMode"})}),(0,i.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,i.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,r)=>{r.d(t,{R:()=>a,x:()=>o});var n=r(6540);const i={},s=n.createContext(i);function a(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);