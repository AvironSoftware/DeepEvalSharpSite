"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[680],{2755:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"metrics/AnswerRelevancyMetric","title":"Answer Relevancy Metric","description":"The Answer Relevancy Metric evaluates how relevant an LLM-generated ActualOutput is in relation to the given InitialInput. This metric is particularly useful for assessing Retrieval-Augmented Generation (RAG) pipelines and ensuring that responses remain on-topic and directly address the input query. The Answer Relevancy Metric provides a reason for its evaluation score, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/AnswerRelevancyMetric.md","sourceDirName":"metrics","slug":"/metrics/AnswerRelevancyMetric","permalink":"/docs/metrics/AnswerRelevancyMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Datasets","permalink":"/docs/datasets"},"next":{"title":"Bias Metric","permalink":"/docs/metrics/BiasMetric"}}');var s=r(4848),i=r(8453);const a={},c="Answer Relevancy Metric",l={},o=[{value:"When you should use Answer Relevancy Metric",id:"when-you-should-use-answer-relevancy-metric",level:4},{value:"When you SHOULDN&#39;T use Answer Relevancy Metric",id:"when-you-shouldnt-use-answer-relevancy-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"answer-relevancy-metric",children:"Answer Relevancy Metric"})}),"\n",(0,s.jsxs)(n.p,{children:["The Answer Relevancy Metric evaluates how relevant an LLM-generated ",(0,s.jsx)(n.code,{children:"ActualOutput"})," is in relation to the given ",(0,s.jsx)(n.code,{children:"InitialInput"}),". This metric is particularly useful for assessing Retrieval-Augmented Generation (RAG) pipelines and ensuring that responses remain on-topic and directly address the input query. The Answer Relevancy Metric provides a reason for its evaluation score, making it a self-explaining LLM-Eval tool."]}),"\n",(0,s.jsx)(n.h4,{id:"when-you-should-use-answer-relevancy-metric",children:"When you should use Answer Relevancy Metric"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Assessing Response Relevance"})," \u2013 Use this metric to ensure an LLM-generated response directly addresses the input without introducing unrelated or off-topic content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimizing RAG Pipelines"})," \u2013 Evaluate how well responses align with retrieved documents, helping refine retrieval strategies."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Benchmarking Model Performance"})," \u2013 Compare different LLMs or iterations of the same model to measure improvements in answer relevancy."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"when-you-shouldnt-use-answer-relevancy-metric",children:"When you SHOULDN'T use Answer Relevancy Metric"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Checking for Fluency or Coherence"})," \u2013 If you need to evaluate language quality, grammatical correctness, or fluency, a different metric is more suitable."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Evaluating Creative or Open-Ended Responses"})," \u2013 If responses are meant to be exploratory or subjective, strict relevancy checks may be too restrictive."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be costly and may not be ideal for high-frequency, large-scale applications."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,s.jsxs)(n.p,{children:["The Answer Relevancy Metric requires ",(0,s.jsx)(n.code,{children:"InitialInput"})," and ",(0,s.jsx)(n.code,{children:"ActualOutput"})," to function. You can instantiate an Answer Relevancy metric with optional parameters to customize its behavior."]}),"\n",(0,s.jsx)(n.p,{children:"Add Answer Relevancy Metric to your evaluator:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"AddAnswerRelevancy(bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,s.jsx)(n.td,{children:"Creates the Answer Relevancy metric and adds it to the evaluator."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"AddAnswerRelevancy(AnswerRelevancyMetricConfiguration config)"})}),(0,s.jsx)(n.td,{children:"Creates the Answer Relevancy metric and adds it to the evaluator."})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Here's an example of how to use Answer Relevancy metric:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "Please summarize the article on climate change impacts.",\r\n        LLMOutput   = "The article talks about how technology is advancing rapidly.",\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        InitialInput    = c.UserInput,\r\n        ActualOutput    = c.LLMOutput\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddAnswerRelevancy(includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,s.jsx)(n.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"InitialInput"})}),(0,s.jsx)(n.td,{children:"A string That represents the initial input is the user interaction with the LLM."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ActualOutput"})}),(0,s.jsx)(n.td,{children:"A string That represents the actual output of the test case from the LLM."})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Threshold"})}),(0,s.jsx)(n.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"IncludeReason"})}),(0,s.jsxs)(n.td,{children:["A boolean that, when set to ",(0,s.jsx)(n.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,s.jsx)(n.code,{children:"True"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"StrictMode"})}),(0,s.jsxs)(n.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,s.jsx)(n.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>c});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);