"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[634],{1459:(e,s,t)=>{t.r(s),t.d(s,{default:()=>i});t(6540);var r=t(1656);const n={navbar:"navbar_Zq8_",logo:"logo_Ukns",navLinks:"navLinks_FO3Z",main:"main_iUjq",hero:"hero_aEcG",title:"title_GqtP",subtitle:"subtitle_RlPM",punchouts:"punchouts_tl8c",icon:"icon_p5U1",features:"features_cAfv",featureGrid:"featureGrid_hfN5",feature:"feature_xuHn",codeSection:"codeSection_XWl9"};var a=t(8774),c=t(4848);function i(){return(0,c.jsx)(r.A,{children:(0,c.jsxs)("main",{className:n.main,children:[(0,c.jsxs)("section",{className:n.hero,children:[(0,c.jsx)("h1",{className:n.title,children:"Test Your LLMs Like a Pro"}),(0,c.jsx)("p",{className:n.subtitle,children:"DeepEvalSharp brings powerful and developer-friendly tools for evaluating large language models directly inside your .NET projects."}),(0,c.jsxs)("div",{className:n.punchouts,children:[(0,c.jsxs)("a",{href:"#",children:[(0,c.jsx)("svg",{className:n.icon,xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",children:(0,c.jsx)("path",{fill:"currentColor",d:"M12 1L3 5v6c0 5.55 3.84 10.74 9 12 5.16-1.26 9-6.45 9-12V5l-9-4z"})}),"MIT License"]}),(0,c.jsxs)("a",{href:"#",children:[(0,c.jsx)("svg",{className:n.icon,xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",children:(0,c.jsx)("path",{fill:"currentColor",d:"M20 10V4a2 2 0 0 0-2-2h-6l-8 8 8 8h6a2 2 0 0 0 2-2v-6z"})}),"v0.1.1-alpha"]})]})]}),(0,c.jsxs)("section",{className:n.features,children:[(0,c.jsx)("h2",{children:"Rapid Evaluations"}),(0,c.jsx)("h3",{children:"With These Robust Metrics"}),(0,c.jsxs)("div",{className:n.featureGrid,children:[(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Answer Relevancy"}),(0,c.jsx)("p",{children:"Measure how relevant the LLM\u2019s response is to the user\u2019s query, ensuring on-topic, directly useful answers. "})]}),(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Bias"}),(0,c.jsx)("p",{children:"Quantifies unwanted prejudice or skew in model outputs so you can detect and mitigate fairness issues."})]}),(0,c.jsxs)(a.A,{to:"docs/ContextualPrecisionMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Contextual Precision"}),(0,c.jsx)("p",{children:"Checks that the retrieved context contains only the most pertinent information for a given query."})]}),(0,c.jsxs)(a.A,{to:"docs/ContextualRecallMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Contextual Recall"}),(0,c.jsx)("p",{children:"Evaluates how comprehensively the retrieval context covers all necessary details to address a query."})]}),(0,c.jsxs)(a.A,{to:"docs/DAGMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"DAG"}),(0,c.jsx)("p",{children:"Uses a decision-tree to break evaluation into atomic checks for fine-grained reliability."})]}),(0,c.jsxs)(a.A,{to:"docs/FaithfulnessMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Faithfulness"}),(0,c.jsx)("p",{children:"Assesses whether outputs are factually supported by the source context, preventing unsupported or fabricated claims."})]}),(0,c.jsxs)(a.A,{to:"docs/GEvalMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"G-Eval"}),(0,c.jsx)("p",{children:"A general-purpose evaluator that leverages chain-of-thought prompting to align LLM quality judgments with human preferences."})]}),(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Hallucination"}),(0,c.jsx)("p",{children:"Detects when the model fabricates or \u201challucinates\u201d information not grounded in the provided context, guarding against false statements."})]}),(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Prompt Alignment"}),(0,c.jsx)("p",{children:"Measures how closely the model\u2019s output follows the structure, style, or instructions specified in the prompt."})]}),(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Summarization"}),(0,c.jsx)("p",{children:"Rates the conciseness, coherence, and completeness of generated summaries relative to the source text."})]}),(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Task Completion"}),(0,c.jsx)("p",{children:"Determines whether the model successfully performs the requested task from end to end, verifying operational effectiveness."})]}),(0,c.jsxs)(a.A,{to:"docs/AnswerRelevancyMetric",className:n.feature,children:[(0,c.jsx)("h4",{children:"Tool Correctnesss"}),(0,c.jsx)("p",{children:"Checks if the model correctly invokes and uses external functions or tools in agentic workflows, ensuring proper tool usage."})]})]})]})]})})}}}]);