"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[634],{6336:(e,s,t)=>{t.r(s),t.d(s,{default:()=>d});var i=t(6540),r=t(1656);const a={navbar:"navbar_Zq8_",logo:"logo_Ukns",navLinks:"navLinks_FO3Z",main:"main_iUjq",content:"content_ewlY",terminalSection:"terminalSection_PtYa",hero:"hero_aEcG",title:"title_GqtP",subtitle:"subtitle_RlPM",punchouts:"punchouts_tl8c",icon:"icon_p5U1",features:"features_cAfv",featureGrid:"featureGrid_hfN5",feature:"feature_xuHn",codeSection:"codeSection_XWl9"};var n=t(8774);const l={terminal:"terminal_Ipi4",terminalheader:"terminalheader_fu1s",trafficlightswrapper:"trafficlightswrapper_BNuV",trafficlights:"trafficlights_l_o_",terminaltitlewrapper:"terminaltitlewrapper_Thua",close:"close_SFPK",minimize:"minimize_kOjZ",maximize:"maximize_wkiN",terminalbody:"terminalbody_onOd",promptline:"promptline_LvmG",promptsymbol:"promptsymbol_ZbfY",terminaloverlay:"terminaloverlay_D7rg",copied:"copied_x5nW",terminaltitle:"terminaltitle_GAN3",terminaltooltip:"terminaltooltip_oX8M"};var c=t(4848);function o(){const e=(0,i.useRef)(null),[s,t]=(0,i.useState)(!1);return(0,c.jsxs)("div",{className:`${l.terminal} ${s?l.copied:""}`,onClick:()=>{navigator.clipboard.writeText("dotnet add package DeepEvalSharp").then((()=>{t(!0),setTimeout((()=>t(!1)),1500)}))},ref:e,children:[(0,c.jsxs)("div",{className:l.terminalheader,children:[(0,c.jsx)("div",{className:l.trafficlightswrapper,children:(0,c.jsxs)("div",{className:l.trafficlights,children:[(0,c.jsx)("span",{className:l.close}),(0,c.jsx)("span",{className:l.minimize}),(0,c.jsx)("span",{className:l.maximize})]})}),(0,c.jsx)("div",{className:l.terminaltitlewrapper,children:(0,c.jsx)("strong",{className:l.terminaltitle,children:"Bash"})})]}),(0,c.jsx)("div",{className:l.terminalbody,children:(0,c.jsxs)("div",{className:l.promptline,children:[(0,c.jsx)("span",{className:l.promptsymbol,children:"$"}),(0,c.jsx)("code",{children:"dotnet add package DeepEvalSharp"})]})}),(0,c.jsx)("div",{className:l.terminaloverlay,children:(0,c.jsx)("span",{children:"Copied!"})}),(0,c.jsx)("div",{className:l.terminaltooltip,children:"Click to copy"})]})}function d(){return(0,c.jsx)(r.A,{children:(0,c.jsxs)("main",{className:`${a.main} ${a.layoutContainer}`,children:[(0,c.jsxs)("div",{className:a.content,children:[(0,c.jsxs)("section",{className:a.hero,children:[(0,c.jsx)("h1",{className:a.title,children:"Test Your LLMs Like a Pro"}),(0,c.jsx)("p",{className:a.subtitle,children:"DeepEvalSharp brings powerful and developer-friendly tools for evaluating large language models directly inside your .NET projects."}),(0,c.jsxs)("div",{className:a.punchouts,children:[(0,c.jsxs)("a",{href:"#",children:[(0,c.jsx)("svg",{className:a.icon,xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",children:(0,c.jsx)("path",{fill:"currentColor",d:"M12 1L3 5v6c0 5.55 3.84 10.74 9 12 5.16-1.26 9-6.45 9-12V5l-9-4z"})}),"MIT License"]}),(0,c.jsxs)("a",{href:"#",children:[(0,c.jsx)("svg",{className:a.icon,xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",children:(0,c.jsx)("path",{fill:"currentColor",d:"M20 10V4a2 2 0 0 0-2-2h-6l-8 8 8 8h6a2 2 0 0 0 2-2v-6z"})}),"v0.1.1-alpha"]})]})]}),(0,c.jsxs)("section",{className:a.features,children:[(0,c.jsx)("h2",{children:"Rapid Evaluations"}),(0,c.jsx)("h3",{children:"With These Robust Metrics"}),(0,c.jsxs)("div",{className:a.featureGrid,children:[(0,c.jsxs)(n.A,{to:"docs/metrics/AnswerRelevancyMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Answer Relevancy"}),(0,c.jsx)("p",{children:"Measure how relevant the LLM\u2019s response is to the user\u2019s query, ensuring on-topic, directly useful answers. "})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/BiasMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Bias"}),(0,c.jsx)("p",{children:"Quantifies unwanted prejudice or skew in model outputs so you can detect and mitigate fairness issues."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/ContextualPrecisionMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Contextual Precision"}),(0,c.jsx)("p",{children:"Checks that the retrieved context contains only the most pertinent information for a given query."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/ContextualRecallMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Contextual Recall"}),(0,c.jsx)("p",{children:"Evaluates how comprehensively the retrieval context covers all necessary details to address a query."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/DAGMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"DAG"}),(0,c.jsx)("p",{children:"Uses a decision-tree to break evaluation into atomic checks for fine-grained reliability."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/FaithfulnessMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Faithfulness"}),(0,c.jsx)("p",{children:"Assesses whether outputs are factually supported by the source context, preventing unsupported or fabricated claims."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/GEvalMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"G-Eval"}),(0,c.jsx)("p",{children:"A general-purpose evaluator that leverages chain-of-thought prompting to align LLM quality judgments with human preferences."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/HallucinationMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Hallucination"}),(0,c.jsx)("p",{children:"Detects when the model fabricates or \u201challucinates\u201d information not grounded in the provided context, guarding against false statements."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/PromptAlignmentMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Prompt Alignment"}),(0,c.jsx)("p",{children:"Measures how closely the model\u2019s output follows the structure, style, or instructions specified in the prompt."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/SummarizationMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Summarization"}),(0,c.jsx)("p",{children:"Rates the conciseness, coherence, and completeness of generated summaries relative to the source text."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/TaskCompletionMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Task Completion"}),(0,c.jsx)("p",{children:"Determines whether the model successfully performs the requested task from end to end, verifying operational effectiveness."})]}),(0,c.jsxs)(n.A,{to:"docs/metrics/ToolCorrectnessMetric",className:a.feature,children:[(0,c.jsx)("h4",{children:"Tool Correctness"}),(0,c.jsx)("p",{children:"Checks if the model correctly invokes and uses external functions or tools in agentic workflows, ensuring proper tool usage."})]})]})]})]}),(0,c.jsx)("div",{className:a.terminalSection,children:(0,c.jsx)(o,{})})]})})}}}]);