"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[664],{3043:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"metrics/TaskCompletionMetric","title":"Task Completion Metric","description":"The Task Completion Metric employs an LLM as a judge to measure how well an agent carries out the task specified in its input, taking into account both the toolscalled and the agent\u2019s actualoutput. The Task Completion Metric is an agentic, referenceless LLM-as-a-judge metric that measures how well an LLM agent accomplishes a user-specified task.","source":"@site/docs/metrics/TaskCompletionMetric.md","sourceDirName":"metrics","slug":"/metrics/TaskCompletionMetric","permalink":"/DeepEvalSharpSite/docs/metrics/TaskCompletionMetric","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/metrics/TaskCompletionMetric.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Summarization Metric","permalink":"/DeepEvalSharpSite/docs/metrics/SummarizationMetric"},"next":{"title":"Tool Correctness Metric","permalink":"/DeepEvalSharpSite/docs/metrics/ToolCorrectnessMetric"}}');var i=s(4848),r=s(8453);const o={},c="Task Completion Metric",a={},l=[{value:"When you should use Task Completion Metric",id:"when-you-should-use-task-completion-metric",level:4},{value:"When you SHOULDN&#39;T use Task Completion Metric",id:"when-you-shouldnt-use-task-completion-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Optional Parameters",id:"optional-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"task-completion-metric",children:"Task Completion Metric"})}),"\n",(0,i.jsxs)(t.p,{children:["The Task Completion Metric employs an LLM as a judge to measure how well an agent carries out the task specified in its ",(0,i.jsx)(t.code,{children:"input"}),", taking into account both the ",(0,i.jsx)(t.code,{children:"tools_called"})," and the agent\u2019s ",(0,i.jsx)(t.code,{children:"actual_output"}),". The Task Completion Metric is an agentic, referenceless LLM-as-a-judge metric that measures how well an LLM agent accomplishes a user-specified task."]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-should-use-task-completion-metric",children:"When you should use Task Completion Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Assessing Agent Effectiveness"})," \u2013 Verify that your LLM agent successfully completes user-defined tasks by invoking necessary tools and providing correct outputs."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Benchmarking Agent Configurations"})," \u2013 Compare different agent strategies, tool sets, or LLM models on their task completion performance."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Debugging Agent Workflows"})," \u2013 Identify under- or over-utilization of tools within your agent pipeline to improve tool integration."]}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-shouldnt-use-task-completion-metric",children:"When you SHOULDN'T use Task Completion Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Non-Agentic Outputs"})," \u2013 This metric is not applicable if your LLM outputs static text without tool calls."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Factual Accuracy Checks"})," \u2013 For pure factual verification, consider Faithfulness or Hallucination metrics instead."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"High-Throughput Requirements"})," \u2013 LLM-as-a-judge evaluations incur API calls and may not suit pipelines where speed and scale are primary concerns."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsxs)(t.p,{children:["The Task Completion Metric requires ",(0,i.jsx)(t.code,{children:"input"}),", ",(0,i.jsx)(t.code,{children:"actual_output"}),", and ",(0,i.jsx)(t.code,{children:"tools_called"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"Instantiate a Task Completion Metric with:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Constructor"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsx)(t.tbody,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"TaskCompletionMetric.Metric()"})}),(0,i.jsx)(t.td,{children:"Initializes a new instance."})]})})]}),"\n",(0,i.jsx)(t.p,{children:"Here\u2019s an example of how to use Task Completion Metric:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-csharp",children:"var metric = TaskCompletionMetric.Metric();\r\nvar result = metric.Evaluate(modelOutput);\n"})}),"\n",(0,i.jsx)(t.h3,{id:"optional-parameters",children:"Optional Parameters"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"threshold"})}),(0,i.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"include_reason"})}),(0,i.jsxs)(t.td,{children:["A boolean that, when set to ",(0,i.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,i.jsx)(t.code,{children:"True"}),"."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"strict_mode"})}),(0,i.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,i.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,s)=>{s.d(t,{R:()=>o,x:()=>c});var n=s(6540);const i={},r=n.createContext(i);function o(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);