"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[6],{8453:(e,r,t)=>{t.d(r,{R:()=>l,x:()=>i});var o=t(6540);const s={},n=o.createContext(s);function l(e){const r=o.useContext(n);return o.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function i(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),o.createElement(n.Provider,{value:r},e.children)}},8754:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>a,contentTitle:()=>i,default:()=>h,frontMatter:()=>l,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"metrics/ToolCorrectnessMetric","title":"Tool Correctness Metric","description":"The Tool Correctness Metric measures how accurately an LLM agent invokes its tools by comparing the actual calls made (ToolsCalled) against the list of expected tools (ExpectedTools). It supports configurable strictness\u2014by default it checks just the tool names, but you can require matching input parameters and outputs for full verification\u2014and returns both a numeric score and a human-readable explanation of any mismatches, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/ToolCorrectnessMetric.md","sourceDirName":"metrics","slug":"/metrics/ToolCorrectnessMetric","permalink":"/EvalSharpSite/docs/metrics/ToolCorrectnessMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Task Completion Metric","permalink":"/EvalSharpSite/docs/metrics/TaskCompletionMetric"}}');var s=t(4848),n=t(8453);const l={},i="Tool Correctness Metric",a={},c=[{value:"When you should use Tool Correctness Metric",id:"when-you-should-use-tool-correctness-metric",level:4},{value:"When you SHOULDN&#39;T use Tool Correctness Metric",id:"when-you-shouldnt-use-tool-correctness-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"tool-correctness-metric",children:"Tool Correctness Metric"})}),"\n",(0,s.jsxs)(r.p,{children:["The Tool Correctness Metric measures how accurately an LLM agent invokes its tools by comparing the actual calls made (",(0,s.jsx)(r.code,{children:"ToolsCalled"}),") against the list of expected tools (",(0,s.jsx)(r.code,{children:"ExpectedTools"}),"). It supports configurable strictness\u2014by default it checks just the tool names, but you can require matching input parameters and outputs for full verification\u2014and returns both a numeric score and a human-readable explanation of any mismatches, making it a self-explaining LLM-Eval tool."]}),"\n",(0,s.jsx)(r.h4,{id:"when-you-should-use-tool-correctness-metric",children:"When you should use Tool Correctness Metric"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Validating Agent Workflows"})," \u2013 Confirm that your agent invokes exactly the tools you intended, catching missed or extra calls."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Benchmarking Tool Integration"})," \u2013 Compare different agent designs or LLM models on their ability to call tools correctly."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Debugging Tool Calls"})," \u2013 Diagnose whether errors stem from incorrect tool usage (wrong parameters, ordering, etc.)."]}),"\n"]}),"\n",(0,s.jsx)(r.h4,{id:"when-you-shouldnt-use-tool-correctness-metric",children:"When you SHOULDN'T use Tool Correctness Metric"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Non-Agentic Text Outputs"})," \u2013 This metric isn\u2019t applicable when your LLM generates standalone text without any tool interactions."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Factual or Relevance Checks"})," \u2013 For assessing content accuracy or topical relevance, use Faithfulness, Hallucination or Answer Relevancy metrics instead."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"High-Throughput Constraints"})," \u2013 Although it\u2019s purely matching logic, large batches of complex test cases may still incur performance considerations."]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,s.jsxs)(r.p,{children:["The Tool Correctness Metric requires ",(0,s.jsx)(r.code,{children:"ToolsCalled"})," and ",(0,s.jsx)(r.code,{children:"ExpectedTools"})," to function. You can instantiate an Tool Correctness metric with optional parameters to customize its behavior."]}),"\n",(0,s.jsx)(r.p,{children:"Add Tool Correctness Metric to your evaluator:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Method"}),(0,s.jsx)(r.th,{children:"Description"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"AddToolCorrectness(bool shouldConsiderOrdering = false, double threshold = 0.5, List<ToolCallParamsEnum>? evaluationParams = null)"})}),(0,s.jsx)(r.td,{children:"Creates the Tool Correctness metric and adds it to the evaluator."})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"AddToolCorrectnessExactMatch(double threshold = 0.5, List<ToolCallParamsEnum>? evaluationParams = null)"})}),(0,s.jsx)(r.td,{children:"Creates the Tool Correctness metric and adds it to the evaluator. Use if only exact matches are desired."})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"AddToolCorrectness(ToolCorrectnessMetricConfiguration config)"})}),(0,s.jsx)(r.td,{children:"Creates the Tool Correctness metric and adds it to the evaluator."})]})]})]}),"\n",(0,s.jsx)(r.p,{children:"Here's an example of how to use Tool Correctness metric:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "Look up product 101 and add it to my cart.",\r\n        LLMOutput   = """{"tool": "product_lookup"} -> {"tool": "add_to_cart"}""",\r\n        ToolsCalled =\r\n        [\r\n            new ToolCall { Name = "product_lookup", InputParameters = new Dictionary<string, object?> { { "product_id", 101 } } },\r\n            new ToolCall\r\n            {\r\n                Name = "add_to_cart",\r\n                InputParameters = new Dictionary<string, object?> { { "product_id", 101 }, { "quantity", 1 } }\r\n            }\r\n        ],\r\n        ExpectedTools =\r\n        [\r\n            new ToolCall { Name = "product_lookup", InputParameters = new Dictionary<string, object?> { { "product_id", 101 } } },\r\n            new ToolCall {\r\n                Name = "add_to_cart",\r\n                InputParameters = new Dictionary<string, object?> { { "product_id", 101 }, { "quantity", 1 } }\r\n            }\r\n        ]\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        ToolsCalled     = c.ToolsCalled,\r\n        ExpectedTools   = c.ExpectedTools\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddToolCorrectness(shouldConsiderOrdering: true, evaluationParams: [ToolCallParamsEnum.INPUT_PARAMETERS, ToolCallParamsEnum.TOOL]);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,s.jsx)(r.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Parameter"}),(0,s.jsx)(r.th,{children:"Description"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ToolsCalled"})}),(0,s.jsxs)(r.td,{children:["A list of ",(0,s.jsx)(r.code,{children:"EvalSharp.Models.ToolCall"}),"'s which are tools your LLM actually invoked during execution."]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ExpectedTools"})}),(0,s.jsxs)(r.td,{children:["A list of ",(0,s.jsx)(r.code,{children:"EvalSharp.Models.ToolCall"}),"'s which are tools your LLM expected tools your LLM should have invoked during execution."]})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Parameter"}),(0,s.jsx)(r.th,{children:"Description"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ShouldConsiderOrdering"})}),(0,s.jsxs)(r.td,{children:["A boolean that indicates if order should be considered apart of the metric score. Default is ",(0,s.jsx)(r.code,{children:"False"}),"."]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"EvaluationParams"})}),(0,s.jsxs)(r.td,{children:["List of tool call parameters to include in scoring; defaults to ",(0,s.jsx)(r.code,{children:"ToolCallParamsEnum.TOOL"}),"."]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ShouldExactMatch"})}),(0,s.jsxs)(r.td,{children:["A boolean that indicates if matching should be exact. If this is provided, ",(0,s.jsx)(r.code,{children:"ShouldConsiderOrdering"})," is ignored, but ",(0,s.jsx)(r.code,{children:"EvaluationParams"})," is still utilized. Default is ",(0,s.jsx)(r.code,{children:"False"}),"."]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"Threshold"})}),(0,s.jsx)(r.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"IncludeReason"})}),(0,s.jsxs)(r.td,{children:["A boolean that, when set to ",(0,s.jsx)(r.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,s.jsx)(r.code,{children:"True"}),"."]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"StrictMode"})}),(0,s.jsxs)(r.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,s.jsx)(r.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:r}={...(0,n.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);