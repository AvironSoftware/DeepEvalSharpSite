"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[634],{7485:(e,s,t)=>{t.r(s),t.d(s,{default:()=>m});var i=t(6540),r=t(1656);const a={navbar:"navbar_Zq8_",logo:"logo_Ukns",navLinks:"navLinks_FO3Z",main:"main_iUjq",top:"top_wZiK",leftSide:"leftSide_mv3t",rightSide:"rightSide_B743",hero:"hero_aEcG",title:"title_GqtP",subtitle:"subtitle_RlPM",buttons:"buttons_AeoN",features:"features_cAfv",featureGrid:"featureGrid_hfN5",rightSideItem:"rightSideItem_ror7",feature:"feature_xuHn",codeSection:"codeSection_XWl9"};var l=t(8774);const c={terminal:"terminal_NVil",terminalheader:"terminalheader_ZFEZ",trafficlightswrapper:"trafficlightswrapper_Tq3B",trafficlights:"trafficlights_gN3l",terminaltitlewrapper:"terminaltitlewrapper_Tn5r",close:"close_sOgX",minimize:"minimize_B_RP",maximize:"maximize_JBNK",terminalbody:"terminalbody_hVs_",promptline:"promptline_rlBL",promptsymbol:"promptsymbol_KByL",terminaloverlay:"terminaloverlay_vav_",copied:"copied_YivM",terminaltitle:"terminaltitle_RyK0",terminaltooltip:"terminaltooltip_ZgO3"};var n=t(4848);function o(){const e=(0,i.useRef)(null),[s,t]=(0,i.useState)(!1);return(0,n.jsxs)("div",{className:`${c.terminal} ${s?c.copied:""}`,onClick:()=>{navigator.clipboard.writeText("dotnet add package DeepEvalSharp").then((()=>{t(!0),setTimeout((()=>t(!1)),1500)}))},ref:e,children:[(0,n.jsxs)("div",{className:c.terminalheader,children:[(0,n.jsx)("div",{className:c.trafficlightswrapper,children:(0,n.jsxs)("div",{className:c.trafficlights,children:[(0,n.jsx)("span",{className:c.close}),(0,n.jsx)("span",{className:c.minimize}),(0,n.jsx)("span",{className:c.maximize})]})}),(0,n.jsx)("div",{className:c.terminaltitlewrapper,children:(0,n.jsx)("strong",{className:c.terminaltitle,children:"Bash"})})]}),(0,n.jsx)("div",{className:c.terminalbody,children:(0,n.jsxs)("div",{className:c.promptline,children:[(0,n.jsx)("span",{className:c.promptsymbol,children:"$"}),(0,n.jsx)("code",{children:"dotnet add package DeepEvalSharp"})]})}),(0,n.jsx)("div",{className:c.terminaloverlay,children:(0,n.jsx)("span",{children:"Copied!"})}),(0,n.jsx)("div",{className:c.terminaltooltip,children:"Click to copy"})]})}const d={punchouts:"punchouts_aT0C",icon:"icon_vZS8"};function h(){return(0,n.jsxs)("div",{className:d.punchouts,children:[(0,n.jsxs)("a",{href:"#",children:[(0,n.jsx)("svg",{className:d.icon,xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",children:(0,n.jsx)("path",{fill:"currentColor",d:"M12 1L3 5v6c0 5.55 3.84 10.74 9 12 5.16-1.26 9-6.45 9-12V5l-9-4z"})}),"MIT License"]}),(0,n.jsxs)("a",{href:"#",children:[(0,n.jsx)("svg",{className:d.icon,xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",children:(0,n.jsx)("path",{fill:"currentColor",d:"M20 10V4a2 2 0 0 0-2-2h-6l-8 8 8 8h6a2 2 0 0 0 2-2v-6z"})}),"v0.1.1-alpha"]})]})}function m(){return(0,n.jsx)(r.A,{children:(0,n.jsxs)("main",{className:a.main,children:[(0,n.jsxs)("div",{className:a.top,children:[(0,n.jsx)("div",{className:a.leftSide,children:(0,n.jsxs)("section",{className:a.hero,children:[(0,n.jsx)("h1",{className:a.title,children:"Test Your LLMs Like a Pro"}),(0,n.jsx)("p",{className:a.subtitle,children:"DeepEvalSharp brings powerful and developer-friendly tools for evaluating large language models directly inside your .NET projects."}),(0,n.jsx)("div",{className:a.buttons,children:(0,n.jsx)(l.A,{className:"button button--primary button--lg",to:"/docs/quick-start",children:"Get Started"})})]})}),(0,n.jsxs)("div",{className:a.rightSide,children:[(0,n.jsx)("div",{className:a.rightSideItem,children:(0,n.jsx)(o,{})}),(0,n.jsx)("div",{className:a.rightSideItem,children:(0,n.jsx)(h,{})})]})]}),(0,n.jsxs)("section",{className:a.features,children:[(0,n.jsx)("h2",{children:"Features"}),(0,n.jsxs)("div",{className:a.featureGrid,children:[(0,n.jsxs)(l.A,{to:"docs/metrics/AnswerRelevancyMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Answer Relevancy"}),(0,n.jsx)("p",{children:"Measure how relevant the LLM\u2019s response is to the user\u2019s query, ensuring on-topic, directly useful answers. "})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/BiasMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Bias"}),(0,n.jsx)("p",{children:"Quantifies unwanted prejudice or skew in model outputs so you can detect and mitigate fairness issues."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/ContextualPrecisionMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Contextual Precision"}),(0,n.jsx)("p",{children:"Checks that the retrieved context contains only the most pertinent information for a given query."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/ContextualRecallMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Contextual Recall"}),(0,n.jsx)("p",{children:"Evaluates how comprehensively the retrieval context covers all necessary details to address a query."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/DAGMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"DAG"}),(0,n.jsx)("p",{children:"Uses a decision-tree to break evaluation into atomic checks for fine-grained reliability."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/FaithfulnessMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Faithfulness"}),(0,n.jsx)("p",{children:"Assesses whether outputs are factually supported by the source context, preventing unsupported or fabricated claims."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/GEvalMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"G-Eval"}),(0,n.jsx)("p",{children:"A general-purpose evaluator that leverages chain-of-thought prompting to align LLM quality judgments with human preferences."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/HallucinationMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Hallucination"}),(0,n.jsx)("p",{children:"Detects when the model fabricates or \u201challucinates\u201d information not grounded in the provided context, guarding against false statements."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/PromptAlignmentMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Prompt Alignment"}),(0,n.jsx)("p",{children:"Measures how closely the model\u2019s output follows the structure, style, or instructions specified in the prompt."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/SummarizationMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Summarization"}),(0,n.jsx)("p",{children:"Rates the conciseness, coherence, and completeness of generated summaries relative to the source text."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/TaskCompletionMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Task Completion"}),(0,n.jsx)("p",{children:"Determines whether the model successfully performs the requested task from end to end, verifying operational effectiveness."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/ToolCorrectnessMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Tool Correctness"}),(0,n.jsx)("p",{children:"Checks if the model correctly invokes and uses external functions or tools in agentic workflows, ensuring proper tool usage."})]}),(0,n.jsxs)(l.A,{to:"docs/metrics/CustomMetric",className:a.feature,children:[(0,n.jsx)("h4",{children:"Custom Metrics"}),(0,n.jsx)("p",{children:"Tailor evaluation logic for specialized or composite LLM use cases."})]})]})]})]})})}}}]);