"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[303],{5361:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"metrics/GEvalMetric","title":"GEval Metric","description":"G-Eval is a framework within NEval that leverages large language models (LLMs) with chain-of-thought (CoT) prompting to assess LLM outputs based on customizable criteria. This versatile metric allows for human-like evaluations across various use cases by defining specific evaluation criteria or steps. Users can create custom metrics by specifying parameters such as \'input\' and \'actualoutput\', and optionally \'expectedoutput\' and \'context\', tailoring the evaluation to their specific needs. G-Eval also offers flexibility in configuration, including options for setting evaluation steps, thresholds, and selecting different LLM models.","source":"@site/docs/metrics/GEvalMetric.md","sourceDirName":"metrics","slug":"/metrics/GEvalMetric","permalink":"/DeepEvalSharpSite/docs/metrics/GEvalMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Faithfulness Metric","permalink":"/DeepEvalSharpSite/docs/metrics/FaithfulnessMetric"},"next":{"title":"Hallucination Metric","permalink":"/DeepEvalSharpSite/docs/metrics/HallucinationMetric"}}');var s=i(4848),r=i(8453);const a={},o="GEval Metric",l={},c=[{value:"When you should use GEval",id:"when-you-should-use-geval",level:4},{value:"When you SHOULDN&#39;T use GEval",id:"when-you-shouldnt-use-geval",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Required Configuration Parameters (Only one is required to be specified)",id:"required-configuration-parameters-only-one-is-required-to-be-specified",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"geval-metric",children:"GEval Metric"})}),"\n",(0,s.jsx)(t.p,{children:"G-Eval is a framework within NEval that leverages large language models (LLMs) with chain-of-thought (CoT) prompting to assess LLM outputs based on customizable criteria. This versatile metric allows for human-like evaluations across various use cases by defining specific evaluation criteria or steps. Users can create custom metrics by specifying parameters such as 'input' and 'actual_output', and optionally 'expected_output' and 'context', tailoring the evaluation to their specific needs. G-Eval also offers flexibility in configuration, including options for setting evaluation steps, thresholds, and selecting different LLM models."}),"\n",(0,s.jsxs)(t.p,{children:["G-Eval came from this paper - and its usage is well described here in the DeepEval docs: ",(0,s.jsx)(t.a,{href:"https://docs.confident-ai.com/docs/metrics-llm-evals#what-is-g-eval",children:"https://docs.confident-ai.com/docs/metrics-llm-evals#what-is-g-eval"})]}),"\n",(0,s.jsx)(t.h4,{id:"when-you-should-use-geval",children:"When you should use GEval"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Automated Human-Like Evaluations"})," \u2013 Use G-Eval to assess LLM responses for coherence, accuracy, and relevance without manual review."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Custom Metrics for Specific Needs"})," \u2013 Define tailored evaluation criteria for domain-specific applications like legal, medical, or creative writing."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Scalable and Flexible Testing"})," \u2013 Configure evaluation steps, thresholds, and model choices for large-scale benchmarking and comparisons."]}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"when-you-shouldnt-use-geval",children:"When you SHOULDN'T use GEval"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Low-Stakes or Simple Evaluations"})," \u2013 If basic accuracy checks or keyword matching suffice, G-Eval's complexity may be unnecessary."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Evaluating Novel or Unpredictable Outputs"})," \u2013 When assessing highly creative or unconventional responses, rigid evaluation criteria may limit fair assessment."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be costly and slow, making G-Eval inefficient for rapid, large-scale testing with limited resources."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,s.jsxs)(t.p,{children:["The GEval Metric requires ",(0,s.jsx)(t.code,{children:"InitialInput"})," and ",(0,s.jsx)(t.code,{children:"ActualOutput"})," to function. You can instantiate an GEval metric with optional parameters to customize its behavior."]}),"\n",(0,s.jsx)(t.p,{children:"Add GEval Metric to your evaluator:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Method"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"AddGEval(string criteria, bool strictMode = false, double threshold = 0.5)"})}),(0,s.jsx)(t.td,{children:"Creates the GEval metric and adds it to the evaluator. The criteria that you specify will be given to an LLM and turned into a set of evaluation steps that the LLM will use to evaluate the model's output."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"AddGEval(IEnumerable<string> evaluationSteps, bool strictMode = false, double threshold = 0.5)"})}),(0,s.jsx)(t.td,{children:"Creates the GEval metric and adds it to the evaluator. The evaluation steps are steps that the LLM will use to evaluate the model's output."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"AddGEval(GEvalMetricConfiguration config)"})}),(0,s.jsx)(t.td,{children:"Creates the GEval metric and adds it to the evaluator."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"Here's an example of how to use GEval metric:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "The United Nations has warned that the global economy is facing severe challenges, with many countries experiencing deep recessions due to the ongoing pandemic. In a new report, the UN highlights the increasing unemployment rates, widespread poverty, and disruptions to supply chains. While some countries are beginning to show signs of recovery, the overall situation remains uncertain. Governments are urged to prioritize fiscal support and sustainable development policies to avoid long-term economic stagnation.",\r\n        LLMOutput   = "The United Nations has issued a warning that the global economy is facing significant challenges, with numerous countries experiencing some very bad things. In a newly released report, the UN emphasizes the rising unemployment rates, increasing levels of poverty, and major disruptions to supply chains that have affected economies worldwide. While certain countries have started to show early signs of recovery, the overall economic outlook remains highly uncertain. The report urges governments to take proactive measures, prioritizing fiscal support and implementing sustainable development policies to prevent long-term economic stagnation and ensure a more stable future.",\r\n        GroundTruth = "The United Nations has warned that the global economy is facing challenges due to the pandemic, with many things that make it difficult for these nations. While some nations are recovering, the global outlook remains uncertain, urging governments to prioritize fiscal support and sustainable development."\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 MetricEvaluationContext\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new MetricEvaluationContext\r\n    {\r\n        InitialInput    = c.UserInput,\r\n        ActualOutput    = c.LLMOutput\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddGEval(criteria: "Is the summary shorter than the original article without omitting key details?", includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,s.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"InitialInput"})}),(0,s.jsx)(t.td,{children:"A string That represents the initial input is the user interaction with the LLM."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"ActualOutput"})}),(0,s.jsx)(t.td,{children:"A string That represents the actual output of the test case from the LLM."})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"required-configuration-parameters-only-one-is-required-to-be-specified",children:"Required Configuration Parameters (Only one is required to be specified)"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"Criteria"})}),(0,s.jsx)(t.td,{children:"A string criteria that you specify will be given to an LLM and turned into a set of evaluation steps that the LLM will use to evaluate the model's output. If EvaluationSteps are provided, this property will be ignored."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"EvaluationSteps"})}),(0,s.jsx)(t.td,{children:"List of strings that represent each step the LLM should use to evaluate"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"Threshold"})}),(0,s.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"StrictMode"})}),(0,s.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,s.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>o});var n=i(6540);const s={},r=n.createContext(s);function a(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);