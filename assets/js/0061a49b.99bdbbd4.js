"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[318],{3782:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"metrics/BiasMetric","title":"Bias Metric","description":"The Bias Metric evaluates whether an LLM-generated ActualOutput contains gender, racial, political, or geographical bias, using an LLM-as-a-judge, referenceless approach to safety and fairness evaluation. It flags biased opinions and provides a reason for its score, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/BiasMetric.md","sourceDirName":"metrics","slug":"/metrics/BiasMetric","permalink":"/EvalSharpSite/docs/metrics/BiasMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Answer Relevancy Metric","permalink":"/EvalSharpSite/docs/metrics/AnswerRelevancyMetric"},"next":{"title":"Contextual Precision Metric","permalink":"/EvalSharpSite/docs/metrics/ContextualPrecisionMetric"}}');var n=r(4848),i=r(8453);const a={},c="Bias Metric",o={},l=[{value:"When you should use Bias Metric",id:"when-you-should-use-bias-metric",level:4},{value:"When you SHOULDN&#39;T use Bias Metric",id:"when-you-shouldnt-use-bias-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"bias-metric",children:"Bias Metric"})}),"\n",(0,n.jsxs)(t.p,{children:["The Bias Metric evaluates whether an LLM-generated ",(0,n.jsx)(t.code,{children:"ActualOutput"})," contains gender, racial, political, or geographical bias, using an LLM-as-a-judge, referenceless approach to safety and fairness evaluation. It flags biased opinions and provides a reason for its score, making it a self-explaining LLM-Eval tool."]}),"\n",(0,n.jsx)(t.h4,{id:"when-you-should-use-bias-metric",children:"When you should use Bias Metric"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Detecting Unintended Bias"})," \u2013 Use this metric to surface gender, racial/ethnic, political, or geographical bias in model outputs."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Monitoring Model Fairness"})," \u2013 Validate that fine-tuning, RLHF, or other optimizations haven\u2019t introduced new biases."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Benchmarking Across Models"})," \u2013 Compare different LLMs or versions on their propensity for biased opinions."]}),"\n"]}),"\n",(0,n.jsx)(t.h4,{id:"when-you-shouldnt-use-bias-metric",children:"When you SHOULDN'T use Bias Metric"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Verifying Factual Accuracy"})," \u2013 This metric does not check facts or correctness, only subjective bias."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Reference-based Comparisons"})," \u2013 If you have a ground-truth \u201cunbiased\u201d reference, a referential metric is more appropriate."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"High-Throughput Environments"})," \u2013 LLM-as-a-judge evaluations incur API calls and may be costly at scale."]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,n.jsxs)(t.p,{children:["The Bias Metric requires ",(0,n.jsx)(t.code,{children:"ActualOutput"})," to function. You can instantiate a Bias metric with optional parameters to customize its behavior."]}),"\n",(0,n.jsx)(t.p,{children:"Add Bias Metric to your evaluator:"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Method"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"AddBias(bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,n.jsx)(t.td,{children:"Creates the Bias metric and adds it to the evaluator."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"AddBias(BiasMetricConfiguration config)"})}),(0,n.jsx)(t.td,{children:"Creates the Bias metric and adds it to the evaluator."})]})]})]}),"\n",(0,n.jsx)(t.p,{children:"Here's an example of how to use Bias metric:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "Who is the best leader in the tech industry?",\r\n        LLMOutput   = "There are many great leaders in the tech industry, such as Satya Nadella, Tim Cook, and Susan Wojcicki."\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        ActualOutput    = c.LLMOutput\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddBias(includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,n.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Parameter"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsx)(t.tbody,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"ActualOutput"})}),(0,n.jsx)(t.td,{children:"A string That represents the actual output of the test case from the LLM."})]})})]}),"\n",(0,n.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Parameter"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"Threshold"})}),(0,n.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"IncludeReason"})}),(0,n.jsxs)(t.td,{children:["A boolean that, when set to ",(0,n.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,n.jsx)(t.code,{children:"True"}),"."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"StrictMode"})}),(0,n.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,n.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,t,r)=>{r.d(t,{R:()=>a,x:()=>c});var s=r(6540);const n={},i=s.createContext(n);function a(e){const t=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);