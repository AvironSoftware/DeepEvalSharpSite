"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[6],{8453:(e,t,s)=>{s.d(t,{R:()=>c,x:()=>i});var r=s(6540);const o={},n=r.createContext(o);function c(e){const t=r.useContext(n);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),r.createElement(n.Provider,{value:t},e.children)}},8754:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>c,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"metrics/ToolCorrectnessMetric","title":"Tool Correctness Metric","description":"The Tool Correctness Metric measures how accurately an LLM agent invokes its tools by comparing the actual calls made (toolscalled) against the list of expected tools (expectedtools). It supports configurable strictness\u2014by default it checks just the tool names, but you can require matching input parameters and outputs for full verification\u2014and returns both a numeric score and a human-readable explanation of any mismatches, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/ToolCorrectnessMetric.md","sourceDirName":"metrics","slug":"/metrics/ToolCorrectnessMetric","permalink":"/DeepEvalSharpSite/docs/metrics/ToolCorrectnessMetric","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/metrics/ToolCorrectnessMetric.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Task Completion Metric","permalink":"/DeepEvalSharpSite/docs/metrics/TaskCompletionMetric"}}');var o=s(4848),n=s(8453);const c={},i="Tool Correctness Metric",l={},a=[{value:"When you should use Tool Correctness Metric",id:"when-you-should-use-tool-correctness-metric",level:4},{value:"When you SHOULDN&#39;T use Tool Correctness Metric",id:"when-you-shouldnt-use-tool-correctness-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Optional Parameters",id:"optional-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"tool-correctness-metric",children:"Tool Correctness Metric"})}),"\n",(0,o.jsxs)(t.p,{children:["The Tool Correctness Metric measures how accurately an LLM agent invokes its tools by comparing the actual calls made (",(0,o.jsx)(t.code,{children:"tools_called"}),") against the list of expected tools (",(0,o.jsx)(t.code,{children:"expected_tools"}),"). It supports configurable strictness\u2014by default it checks just the tool names, but you can require matching input parameters and outputs for full verification\u2014and returns both a numeric score and a human-readable explanation of any mismatches, making it a self-explaining LLM-Eval tool."]}),"\n",(0,o.jsx)(t.h4,{id:"when-you-should-use-tool-correctness-metric",children:"When you should use Tool Correctness Metric"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Validating Agent Workflows"})," \u2013 Confirm that your agent invokes exactly the tools you intended, catching missed or extra calls."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Benchmarking Tool Integration"})," \u2013 Compare different agent designs or LLM models on their ability to call tools correctly."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Debugging Tool Calls"})," \u2013 Diagnose whether errors stem from incorrect tool usage (wrong parameters, ordering, etc.)."]}),"\n"]}),"\n",(0,o.jsx)(t.h4,{id:"when-you-shouldnt-use-tool-correctness-metric",children:"When you SHOULDN'T use Tool Correctness Metric"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Non-Agentic Text Outputs"})," \u2013 This metric isn\u2019t applicable when your LLM generates standalone text without any tool interactions."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Factual or Relevance Checks"})," \u2013 For assessing content accuracy or topical relevance, use Faithfulness, Hallucination or Answer Relevancy metrics instead."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"High-Throughput Constraints"})," \u2013 Although it\u2019s purely matching logic, large batches of complex test cases may still incur performance considerations."]}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,o.jsx)(t.p,{children:"The Tool Correctness Metric requires four fields:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.code,{children:"input"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.code,{children:"actual_output"})}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.code,{children:"tools_called"})," \u2013 list of ",(0,o.jsx)(t.code,{children:"ToolCall"})," objects actually invoked"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.code,{children:"expected_tools"})," \u2013 list of ",(0,o.jsx)(t.code,{children:"ToolCall"})," objects you expect the agent to invoke"]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"Instantiate the metric with:"}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{children:"Constructor"}),(0,o.jsx)(t.th,{children:"Description"})]})}),(0,o.jsx)(t.tbody,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:(0,o.jsx)(t.code,{children:"ToolCorrectnessMetric.Metric()"})}),(0,o.jsx)(t.td,{children:"Creates a new Tool Correctness instance"})]})})]}),"\n",(0,o.jsx)(t.p,{children:"Here\u2019s an example of how to use Tool Correctness Metric:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-csharp",children:"var metric = ToolCorrectnessMetric.Metric();\r\nvar result = metric.Evaluate(modelOutput);\n"})}),"\n",(0,o.jsx)(t.h3,{id:"optional-parameters",children:"Optional Parameters"}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{children:"Parameter"}),(0,o.jsx)(t.th,{children:"Description"})]})}),(0,o.jsxs)(t.tbody,{children:[(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:(0,o.jsx)(t.code,{children:"threshold"})}),(0,o.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:(0,o.jsx)(t.code,{children:"include_reason"})}),(0,o.jsxs)(t.td,{children:["A boolean that, when set to ",(0,o.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,o.jsx)(t.code,{children:"True"}),"."]})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:(0,o.jsx)(t.code,{children:"strict_mode"})}),(0,o.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,o.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);