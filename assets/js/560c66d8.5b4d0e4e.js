"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[649],{1586:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"metrics/FaithfulnessMetric","title":"Faithfulness Metric","description":"The Faithfulness Metric assesses the quality of your Retrieval-Augmented Generation (RAG) pipeline\'s generator by evaluating whether the ActualOutput factually aligns with the contents of your RetrievalContext. This metric focuses on identifying contradictions between the generated output and the provided context, ensuring that the information presented is accurate and trustworthy. Additionally, it offers explanations for its scores, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/FaithfulnessMetric.md","sourceDirName":"metrics","slug":"/metrics/FaithfulnessMetric","permalink":"/docs/metrics/FaithfulnessMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Custom Metric","permalink":"/docs/metrics/CustomMetric"},"next":{"title":"GEval Metric","permalink":"/docs/metrics/GEvalMetric"}}');var i=n(4848),s=n(8453);const a={},o="Faithfulness Metric",l={},c=[{value:"When you should use Faithfulness Metric",id:"when-you-should-use-faithfulness-metric",level:4},{value:"When you SHOULDN&#39;T use Faithfulness Metric",id:"when-you-shouldnt-use-faithfulness-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"faithfulness-metric",children:"Faithfulness Metric"})}),"\n",(0,i.jsxs)(t.p,{children:["The Faithfulness Metric assesses the quality of your Retrieval-Augmented Generation (RAG) pipeline's generator by evaluating whether the ",(0,i.jsx)(t.code,{children:"ActualOutput"})," factually aligns with the contents of your ",(0,i.jsx)(t.code,{children:"RetrievalContext"}),". This metric focuses on identifying contradictions between the generated output and the provided context, ensuring that the information presented is accurate and trustworthy. Additionally, it offers explanations for its scores, making it a self-explaining LLM-Eval tool."]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-should-use-faithfulness-metric",children:"When you should use Faithfulness Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Ensuring Output Accuracy"})," \u2013 Use this metric to verify that the generated responses are factually consistent with the retrieved context, minimizing misinformation."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Evaluating RAG Pipeline Integrity"})," \u2013 Assess the reliability of your RAG pipeline by ensuring that the generator produces outputs faithful to the retrieved information."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Identifying Contradictions"})," \u2013 Detect and address any discrepancies between the generated content and the source material to maintain credibility."]}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-shouldnt-use-faithfulness-metric",children:"When you SHOULDN'T use Faithfulness Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Assessing Language Quality"})," \u2013 If your goal is to evaluate the fluency, coherence, or stylistic aspects of the generated text, other metrics like Answer Relevancy or Summarization may be more appropriate."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Evaluating Retrieval Performance"})," \u2013 When focusing on the effectiveness of the retriever in fetching relevant documents, metrics like Contextual Precision or Contextual Recall would be more suitable."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be computationally intensive and may not be ideal for large-scale or real-time applications with limited resources."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsxs)(t.p,{children:["The Faithfulness Metric requires ",(0,i.jsx)(t.code,{children:"ActualOutput"})," and ",(0,i.jsx)(t.code,{children:"RetrievalContext"})," to function. You can instantiate an Faithfulness metric with optional parameters to customize its behavior."]}),"\n",(0,i.jsx)(t.p,{children:"Add Faithfulness Metric to your evaluator:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Method"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddFaithfulness(int? truthsExtractionLimit = null, bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,i.jsx)(t.td,{children:"Creates the Faithfulness metric and adds it to the evaluator. You can optionally set the number of truths to extract from the actual output. Leaving this NULL will let the LLM decide"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddFaithfulness(FaithfulnessMetricConfiguration config)"})}),(0,i.jsx)(t.td,{children:"Creates the Faithfulness metric and adds it to the evaluator."})]})]})]}),"\n",(0,i.jsx)(t.p,{children:"Here's an example of how to use Faithfulness metric:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "I need help with my account; I mentioned earlier that my account number is 12345, but I cannot log in.",\r\n        LLMOutput   = "Your account number is 12345. Try resetting your password using the \'Forgot Password\' link.",\r\n        GroundTruth = "Since you mentioned account number 12345 earlier and stated you\'re having trouble logging in, please try resetting your password using the \'Forgot Password\' link. If the issue persists, let me escalate your ticket for further assistance.",\r\n        RetrievalContext =\r\n        [\r\n            "Previous conversation where the user mentioned account number 12345 and issues with logging in."\r\n        ]\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        ActualOutput    = c.LLMOutput,\r\n        RetrievalContext = c.RetrievalContext\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddFaithfulness(includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,i.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"ActualOutput"})}),(0,i.jsx)(t.td,{children:"A string That represents the actual output of the test case from the LLM."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"RetrievalContext"})}),(0,i.jsx)(t.td,{children:"A list of background information strings that your app actually found when answering. Use this to compare what was retrieved against the ideal Context."})]})]})]}),"\n",(0,i.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"TruthsExtractionLimit"})}),(0,i.jsx)(t.td,{children:"An integer specifying the number of truths to extract from the actual output. Leaving this NULL will let the LLM decide."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"Threshold"})}),(0,i.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"IncludeReason"})}),(0,i.jsxs)(t.td,{children:["A boolean that, when set to ",(0,i.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,i.jsx)(t.code,{children:"True"}),"."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"StrictMode"})}),(0,i.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,i.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var r=n(6540);const i={},s=r.createContext(i);function a(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);