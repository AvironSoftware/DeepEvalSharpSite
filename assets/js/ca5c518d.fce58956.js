"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[109],{8453:(e,s,t)=>{t.d(s,{R:()=>r,x:()=>l});var a=t(6540);const i={},n=a.createContext(i);function r(e){const s=a.useContext(n);return a.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(n.Provider,{value:s},e.children)}},9258:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"GEvalMetric","title":"GEval Metric","description":"G-Eval is a framework within NEval that leverages large language models (LLMs) with chain-of-thought (CoT) prompting to assess LLM outputs based on customizable criteria. This versatile metric allows for human-like evaluations across various use cases by defining specific evaluation criteria or steps. Users can create custom metrics by specifying parameters such as \'input\' and \'actualoutput\', and optionally \'expectedoutput\' and \'context\', tailoring the evaluation to their specific needs. G-Eval also offers flexibility in configuration, including options for setting evaluation steps, thresholds, and selecting different LLM models.","source":"@site/docs/GEvalMetric.md","sourceDirName":".","slug":"/GEvalMetric","permalink":"/DeepEvalSharpSite/docs/GEvalMetric","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/GEvalMetric.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Faithfulness Metric","permalink":"/DeepEvalSharpSite/docs/FaithfulnessMetric"}}');var i=t(4848),n=t(8453);const r={},l="GEval Metric",o={},c=[{value:"When you should use GEval",id:"when-you-should-use-geval",level:4},{value:"When you SHOULDN&#39;T use GEval",id:"when-you-shouldnt-use-geval",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Samples",id:"samples",level:2}];function d(e){const s={a:"a",code:"code",h1:"h1",h2:"h2",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"geval-metric",children:"GEval Metric"})}),"\n",(0,i.jsx)(s.p,{children:"G-Eval is a framework within NEval that leverages large language models (LLMs) with chain-of-thought (CoT) prompting to assess LLM outputs based on customizable criteria. This versatile metric allows for human-like evaluations across various use cases by defining specific evaluation criteria or steps. Users can create custom metrics by specifying parameters such as 'input' and 'actual_output', and optionally 'expected_output' and 'context', tailoring the evaluation to their specific needs. G-Eval also offers flexibility in configuration, including options for setting evaluation steps, thresholds, and selecting different LLM models."}),"\n",(0,i.jsxs)(s.p,{children:["G-Eval came from this paper - and its usage is well described here in the DeepEval docs: ",(0,i.jsx)(s.a,{href:"https://docs.confident-ai.com/docs/metrics-llm-evals#what-is-g-eval",children:"https://docs.confident-ai.com/docs/metrics-llm-evals#what-is-g-eval"})]}),"\n",(0,i.jsx)(s.h4,{id:"when-you-should-use-geval",children:"When you should use GEval"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Automated Human-Like Evaluations"})," \u2013 Use G-Eval to assess LLM responses for coherence, accuracy, and relevance without manual review."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Custom Metrics for Specific Needs"})," \u2013 Define tailored evaluation criteria for domain-specific applications like legal, medical, or creative writing."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Scalable and Flexible Testing"})," \u2013 Configure evaluation steps, thresholds, and model choices for large-scale benchmarking and comparisons."]}),"\n"]}),"\n",(0,i.jsx)(s.h4,{id:"when-you-shouldnt-use-geval",children:"When you SHOULDN'T use GEval"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Low-Stakes or Simple Evaluations"})," \u2013 If basic accuracy checks or keyword matching suffice, G-Eval's complexity may be unnecessary."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Evaluating Novel or Unpredictable Outputs"})," \u2013 When assessing highly creative or unconventional responses, rigid evaluation criteria may limit fair assessment."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be costly and slow, making G-Eval inefficient for rapid, large-scale testing with limited resources."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsx)(s.p,{children:"GEval requires either criteria or evaluation steps."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Criteria is useful when doing quick evaluations of LLM-generated outputs for relevance and factual accuracy, ensuring they meet broad quality standards without requiring step-by-step reasoning."}),"\n",(0,i.jsx)(s.li,{children:"Evaluation steps is useful when you need to systematically verify complex outputs, such as fact-checking a legal argument by breaking down claims and cross-referencing reliable sources."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Instantiate a GEval metric by using one of these static constructors:"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Constructor"}),(0,i.jsx)(s.th,{children:"Description"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"GEval.ForCriteria(string criteria)"})}),(0,i.jsx)(s.td,{children:"Create a GEval metric for a specific criteria. NOTE: this makes an additional call to the LLM to generate evaluation steps based on the initial criteria."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"GEval.ForEvaluationSteps(List<string> evaluationSteps)"})}),(0,i.jsx)(s.td,{children:"Create a GEval metric with a list of evaluation steps."})]})]})]}),"\n",(0,i.jsx)(s.p,{children:"Here's an example of how to use GEval:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-csharp",children:'var metric = GEval.ForCriteria("bla bla bla");\r\nvar result = metric.Evaluate(modelOutput);\n'})}),"\n",(0,i.jsx)(s.p,{children:"[[The optional parameters are: (these are obv incorrect, just an example)]]"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Parameter"}),(0,i.jsx)(s.th,{children:"Description"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"threshold"})}),(0,i.jsx)(s.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"StrictMode"})}),(0,i.jsx)(s.td,{children:"A boolean that, when set to True, enforces a binary metric score\u20141 for perfection, 0 otherwise\u2014and sets the threshold to 1. The default is False"})]})]})]}),"\n",(0,i.jsx)(s.h2,{id:"samples",children:"Samples"}),"\n",(0,i.jsx)(s.p,{children:"We've given you a sample to work with that evaluates bla bla bla. Here's how you can run these samples:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-csharp",children:'var metric = GEval.ForCriteria("bla bla bla");\r\nvar result = metric.Evaluate(modelOutput);\n'})})]})}function h(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);