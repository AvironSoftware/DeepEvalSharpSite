"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[79],{1497:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"metrics/SummarizationMetric","title":"Summarization Metric","description":"The Summarization Metric leverages an LLM as a judge to assess if your model\u2019s actual_output is both factually accurate and sufficiently comprehensive when compared to the original input. It generates two sub-scores\u2014a contradiction-and-fabrication\u2013detecting alignment score and a key-information\u2013measuring coverage score\u2014takes the lower of the two as the final result, and supplies a human-readable explanation for its judgment, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/SummarizationMetric.md","sourceDirName":"metrics","slug":"/metrics/SummarizationMetric","permalink":"/DeepEvalSharpSite/docs/metrics/SummarizationMetric","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/metrics/SummarizationMetric.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Prompt Alignment Metric","permalink":"/DeepEvalSharpSite/docs/metrics/PromptAlignmentMetric"},"next":{"title":"Task Completion Metric","permalink":"/DeepEvalSharpSite/docs/metrics/TaskCompletionMetric"}}');var n=i(4848),s=i(8453);const a={},o="Summarization Metric",c={},l=[{value:"When you should use Summarization Metric",id:"when-you-should-use-summarization-metric",level:4},{value:"When you SHOULDN&#39;T use Summarization Metric",id:"when-you-shouldnt-use-summarization-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Optional Parameters",id:"optional-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"summarization-metric",children:"Summarization Metric"})}),"\n",(0,n.jsxs)(t.p,{children:["The Summarization Metric leverages an LLM as a judge to assess if your model\u2019s ",(0,n.jsx)(t.code,{children:"actual_output"})," is both factually accurate and sufficiently comprehensive when compared to the original ",(0,n.jsx)(t.code,{children:"input"}),". It generates two sub-scores\u2014a contradiction-and-fabrication\u2013detecting alignment score and a key-information\u2013measuring coverage score\u2014takes the lower of the two as the final result, and supplies a human-readable explanation for its judgment, making it a self-explaining LLM-Eval tool."]}),"\n",(0,n.jsx)(t.h4,{id:"when-you-should-use-summarization-metric",children:"When you should use Summarization Metric"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Assessing Summary Fidelity"})," \u2013 Ensure summaries accurately reflect source text without fabrications or contradictions."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Evaluating Information Coverage"})," \u2013 Verify that the summary includes all necessary details from the original content."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Benchmarking Summarization Quality"})," \u2013 Compare different models or configurations on their ability to generate faithful, comprehensive summaries."]}),"\n"]}),"\n",(0,n.jsx)(t.h4,{id:"when-you-shouldnt-use-summarization-metric",children:"When you SHOULDN'T use Summarization Metric"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Fact-Checking with Explicit References"})," \u2013 For context-backed validation, metrics like Hallucination or Faithfulness are more appropriate."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Creative or Abstracted Summaries"})," \u2013 If you require imaginative rewrites rather than faithful representation."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"High-Throughput/Cacheable Requirements"})," \u2013 As a non-cacheable metric requiring multiple LLM calls, it may not suit large-scale pipelines."]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,n.jsxs)(t.p,{children:["The Summarization Metric requires",(0,n.jsx)(t.code,{children:"input"})," and ",(0,n.jsx)(t.code,{children:"actual_output"}),". Instantiate and run it as follows:"]}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Constructor"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsx)(t.tbody,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"SummarizationMetric.Metric()"})}),(0,n.jsx)(t.td,{children:"Initializes a new instance of the metric."})]})})]}),"\n",(0,n.jsx)(t.p,{children:"Here\u2019s an example of how to use Summarization Metric:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-csharp",children:"var metric = SummarizationMetric.Metric();\r\nvar result = metric.Evaluate(modelOutput);\n"})}),"\n",(0,n.jsx)(t.h3,{id:"optional-parameters",children:"Optional Parameters"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Parameter"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"threshold"})}),(0,n.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"include_reason"})}),(0,n.jsxs)(t.td,{children:["A boolean that, when set to ",(0,n.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,n.jsx)(t.code,{children:"True"}),"."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"strict_mode"})}),(0,n.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,n.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>o});var r=i(6540);const n={},s=r.createContext(n);function a(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);