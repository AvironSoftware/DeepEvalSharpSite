"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[171],{8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var r=n(6540);const i={},s=r.createContext(i);function o(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:t},e.children)}},8598:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"metrics/ContextualPrecisionMetric","title":"Contextual Precision Metric","description":"The Contextual Precision Metric evaluates how well a Retrieval-Augmented Generation (RAG) pipeline\'s retriever ranks relevant context higher than irrelevant context for a given input. This metric helps ensure that an LLM receives the most useful information, improving the accuracy and quality of generated responses. The Contextual Precision Metric provides an explanation for its evaluation score, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/ContextualPrecisionMetric.md","sourceDirName":"metrics","slug":"/metrics/ContextualPrecisionMetric","permalink":"/DeepEvalSharpSite/docs/metrics/ContextualPrecisionMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Bias Metric","permalink":"/DeepEvalSharpSite/docs/metrics/BiasMetric"},"next":{"title":"Contextual Recall Metric","permalink":"/DeepEvalSharpSite/docs/metrics/ContextualRecallMetric"}}');var i=n(4848),s=n(8453);const o={},a="Contextual Precision Metric",c={},l=[{value:"When you should use Contextual Precision Metric",id:"when-you-should-use-contextual-precision-metric",level:4},{value:"When you SHOULDN&#39;T use Contextual Precision Metric",id:"when-you-shouldnt-use-contextual-precision-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"contextual-precision-metric",children:"Contextual Precision Metric"})}),"\n",(0,i.jsxs)(t.p,{children:["The Contextual Precision Metric evaluates how well a Retrieval-Augmented Generation (RAG) pipeline's retriever ranks relevant context higher than irrelevant context for a given ",(0,i.jsx)(t.code,{children:"input"}),". This metric helps ensure that an LLM receives the most useful information, improving the accuracy and quality of generated responses. The Contextual Precision Metric provides an explanation for its evaluation score, making it a self-explaining LLM-Eval tool."]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-should-use-contextual-precision-metric",children:"When you should use Contextual Precision Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Evaluating Retriever Performance"})," \u2013 Use this metric to assess whether relevant documents or context appear at the top of retrieved results."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Optimizing Retrieval Strategies"})," \u2013 Identify and refine retrieval techniques to ensure LLMs receive high-quality supporting information."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Improving Re-Ranking Algorithms"})," \u2013 Measure how well re-ranking methods prioritize relevant data over irrelevant information."]}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-shouldnt-use-contextual-precision-metric",children:"When you SHOULDN'T use Contextual Precision Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Assessing LLM Response Quality"})," \u2013 This metric evaluates context ranking, not the coherence or accuracy of generated text."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Measuring Recall Instead of Precision"})," \u2013 If you need to ensure all relevant information is retrieved, consider using a recall-based metric instead."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be computationally intensive and may not be ideal for large-scale applications."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsxs)(t.p,{children:["The Contextual Precision Metric requires ",(0,i.jsx)(t.code,{children:"InitialInput"}),", ",(0,i.jsx)(t.code,{children:"ExpectedOutput"}),", and ",(0,i.jsx)(t.code,{children:"RetrievalContext"})," to function. You can instantiate an Contextual Precision metric with optional parameters to customize its behavior."]}),"\n",(0,i.jsx)(t.p,{children:"Add Contextual Precision Metric to your evaluator:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Method"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddContextualPrecision(bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,i.jsx)(t.td,{children:"Creates the Contextual Precision metric and adds it to the evaluator."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddContextualPrecision(ContextualPrecisionMetricConfiguration config)"})}),(0,i.jsx)(t.td,{children:"Creates the Contextual Precision metric and adds it to the evaluator."})]})]})]}),"\n",(0,i.jsx)(t.p,{children:"Here's an example of how to use Contextual Precision metric:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "I need help with my account; I mentioned earlier that my account number is 12345, but I cannot log in.",\r\n        LLMOutput   = "Please reset your password using the forgot password link.",\r\n        GroundTruth   = "Since you mentioned account number 12345 earlier and stated you\'re having trouble logging in, please try resetting your password using the \'Forgot Password\' link. If the issue persists, let me escalate your ticket for further assistance.",\r\n        RetrievalContext = ["Previous conversation where the user mentioned account number 12345 and issues with logging in."]\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        InitialInput    = c.UserInput,\r\n        ExpectedOutput    = c.GroundTruth,\r\n        RetrievalContext = c.RetrievalContext\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddContextualPrecision(includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,i.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"InitialInput"})}),(0,i.jsx)(t.td,{children:"A string That represents the initial input is the user interaction with the LLM."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"ExpectedOutput"})}),(0,i.jsx)(t.td,{children:"The expected output of the test case."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"RetrievalContext"})}),(0,i.jsx)(t.td,{children:"A list of background information strings that your app actually found when answering. Use this to compare what was retrieved against the ideal Context."})]})]})]}),"\n",(0,i.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"Threshold"})}),(0,i.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"IncludeReason"})}),(0,i.jsxs)(t.td,{children:["A boolean that, when set to ",(0,i.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,i.jsx)(t.code,{children:"True"}),"."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"StrictMode"})}),(0,i.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,i.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);