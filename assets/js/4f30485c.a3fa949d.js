"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[52],{8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>a});var r=n(6540);const i={},s=r.createContext(i);function l(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(s.Provider,{value:t},e.children)}},9979:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"metrics/ContextualRecallMetric","title":"Contextual Recall Metric","description":"The Contextual Recall Metric evaluates the effectiveness of your Retrieval-Augmented Generation (RAG) pipeline\'s retriever by assessing how well the retrieved context (RetrievalContext) aligns with the expected output (ExpectedOutput). This metric ensures that the retriever captures and provides all relevant information necessary for generating accurate responses. Additionally, it offers explanations for its scores, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/ContextualRecallMetric.md","sourceDirName":"metrics","slug":"/metrics/ContextualRecallMetric","permalink":"/DeepEvalSharpSite/docs/metrics/ContextualRecallMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Contextual Precision Metric","permalink":"/DeepEvalSharpSite/docs/metrics/ContextualPrecisionMetric"},"next":{"title":"Custom Metric","permalink":"/DeepEvalSharpSite/docs/metrics/CustomMetric"}}');var i=n(4848),s=n(8453);const l={},a="Contextual Recall Metric",o={},c=[{value:"When you should use Contextual Recall Metric",id:"when-you-should-use-contextual-recall-metric",level:4},{value:"When you SHOULDN&#39;T use Contextual Recall Metric",id:"when-you-shouldnt-use-contextual-recall-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"contextual-recall-metric",children:"Contextual Recall Metric"})}),"\n",(0,i.jsxs)(t.p,{children:["The Contextual Recall Metric evaluates the effectiveness of your Retrieval-Augmented Generation (RAG) pipeline's retriever by assessing how well the retrieved context (",(0,i.jsx)(t.code,{children:"RetrievalContext"}),") aligns with the expected output (",(0,i.jsx)(t.code,{children:"ExpectedOutput"}),"). This metric ensures that the retriever captures and provides all relevant information necessary for generating accurate responses. Additionally, it offers explanations for its scores, making it a self-explaining LLM-Eval tool."]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-should-use-contextual-recall-metric",children:"When you should use Contextual Recall Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Evaluating Retriever Coverage"})," \u2013 Use this metric to assess whether your retriever captures all necessary information from the knowledge base to generate the expected output."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Optimizing Embedding Models"})," \u2013 Determine if your embedding model accurately represents and retrieves relevant information based on the input context."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Improving Retrieval Strategies"})," \u2013 Identify gaps in the retrieval process to ensure comprehensive information is provided to the generator."]}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"when-you-shouldnt-use-contextual-recall-metric",children:"When you SHOULDN'T use Contextual Recall Metric"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Assessing Response Quality"})," \u2013 If your goal is to evaluate the quality, fluency, or coherence of the generated responses, other metrics like Answer Relevancy or Faithfulness may be more appropriate."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Evaluating Ranking Performance"})," \u2013 When focusing on the order of retrieved information, the Contextual Precision Metric would be more suitable."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be computationally intensive and may not be ideal for large-scale or real-time applications with limited resources."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsxs)(t.p,{children:["The Contextual Recall Metric requires ",(0,i.jsx)(t.code,{children:"ExpectedOutput"})," and ",(0,i.jsx)(t.code,{children:"RetrievalContext"})," to function. You can instantiate an Contextual Recall metric with optional parameters to customize its behavior."]}),"\n",(0,i.jsx)(t.p,{children:"Add Contextual Recall Metric to your evaluator:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Method"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddContextualRecall(bool strictMode = false, double threshold = 0.5)"})}),(0,i.jsx)(t.td,{children:"Creates the Contextual Recall metric and adds it to the evaluator."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"AddContextualRecall(ContextualRecallMetricConfiguration config)"})}),(0,i.jsx)(t.td,{children:"Creates the Contextual Recall metric and adds it to the evaluator."})]})]})]}),"\n",(0,i.jsx)(t.p,{children:"Here's an example of how to use Contextual Recall metric:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "Can you explain why plants need sunlight, referring to our discussion on photosynthesis last week?",\r\n        LLMOutput   = "Plants need sunlight because it helps them grow more flowers.",\r\n        GroundTruth   = "As discussed in our previous lesson on photosynthesis, plants require sunlight to convert carbon dioxide and water into glucose and oxygen, which is vital for their growth and energy production.",\r\n        RetrievalContext =\r\n        [\r\n            "Previous lesson included a detailed explanation of photosynthesis and the role of sunlight in the process.",\r\n            "Irrelevant context: Some sources incorrectly claim that sunlight only influences the blooming of flowers."\r\n        ]\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        InitialInput    = "",\r\n        ActualOutput    = "",\r\n        ExpectedOutput = c.GroundTruth,\r\n        RetrievalContext = c.RetrievalContext\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddContextualRecall(includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,i.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"ExpectedOutput"})}),(0,i.jsx)(t.td,{children:"The expected output of the test case."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"RetrievalContext"})}),(0,i.jsx)(t.td,{children:"A list of background information strings that your app actually found when answering. Use this to compare what was retrieved against the ideal Context."})]})]})]}),"\n",(0,i.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Parameter"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"Threshold"})}),(0,i.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"StrictMode"})}),(0,i.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,i.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);