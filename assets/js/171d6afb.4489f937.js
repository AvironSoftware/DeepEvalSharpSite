"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[171],{8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var i=n(6540);const r={},s=i.createContext(r);function o(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:t},e.children)}},8598:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"metrics/ContextualPrecisionMetric","title":"Contextual Precision Metric","description":"The Contextual Precision Metric evaluates how well a Retrieval-Augmented Generation (RAG) pipeline\'s retriever ranks relevant context higher than irrelevant context for a given input. This metric helps ensure that an LLM receives the most useful information, improving the accuracy and quality of generated responses. The Contextual Precision Metric provides an explanation for its evaluation score, making it a self-explaining LLM-Eval tool.","source":"@site/docs/metrics/ContextualPrecisionMetric.md","sourceDirName":"metrics","slug":"/metrics/ContextualPrecisionMetric","permalink":"/DeepEvalSharpSite/docs/metrics/ContextualPrecisionMetric","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/metrics/ContextualPrecisionMetric.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Bias Metric","permalink":"/DeepEvalSharpSite/docs/metrics/BiasMetric"},"next":{"title":"Contextual Recall Metric","permalink":"/DeepEvalSharpSite/docs/metrics/ContextualRecallMetric"}}');var r=n(4848),s=n(8453);const o={},a="Contextual Precision Metric",c={},l=[{value:"When you should use Contextual Precision Metric",id:"when-you-should-use-contextual-precision-metric",level:4},{value:"When you SHOULDN&#39;T use Contextual Precision Metric",id:"when-you-shouldnt-use-contextual-precision-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Optional Parameters",id:"optional-parameters",level:3},{value:"Samples",id:"samples",level:2}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"contextual-precision-metric",children:"Contextual Precision Metric"})}),"\n",(0,r.jsxs)(t.p,{children:["The Contextual Precision Metric evaluates how well a Retrieval-Augmented Generation (RAG) pipeline's retriever ranks relevant context higher than irrelevant context for a given ",(0,r.jsx)(t.code,{children:"input"}),". This metric helps ensure that an LLM receives the most useful information, improving the accuracy and quality of generated responses. The Contextual Precision Metric provides an explanation for its evaluation score, making it a self-explaining LLM-Eval tool."]}),"\n",(0,r.jsx)(t.h4,{id:"when-you-should-use-contextual-precision-metric",children:"When you should use Contextual Precision Metric"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Evaluating Retriever Performance"})," \u2013 Use this metric to assess whether relevant documents or context appear at the top of retrieved results."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Optimizing Retrieval Strategies"})," \u2013 Identify and refine retrieval techniques to ensure LLMs receive high-quality supporting information."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Improving Re-Ranking Algorithms"})," \u2013 Measure how well re-ranking methods prioritize relevant data over irrelevant information."]}),"\n"]}),"\n",(0,r.jsx)(t.h4,{id:"when-you-shouldnt-use-contextual-precision-metric",children:"When you SHOULDN'T use Contextual Precision Metric"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Assessing LLM Response Quality"})," \u2013 This metric evaluates context ranking, not the coherence or accuracy of generated text."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Measuring Recall Instead of Precision"})," \u2013 If you need to ensure all relevant information is retrieved, consider using a recall-based metric instead."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Resource-Constrained Environments"})," \u2013 Running LLM-based evaluations can be computationally intensive and may not be ideal for large-scale applications."]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,r.jsxs)(t.p,{children:["The Contextual Precision Metric requires ",(0,r.jsx)(t.code,{children:"input"}),", ",(0,r.jsx)(t.code,{children:"actual_output"}),", ",(0,r.jsx)(t.code,{children:"expected_output"}),", and ",(0,r.jsx)(t.code,{children:"retrieval_context"}),". You can instantiate the metric with optional parameters to customize its behavior."]}),"\n",(0,r.jsx)(t.p,{children:"Instantiate a Contextual Precision Metric by using one of these static constructors:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Constructor"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsx)(t.tbody,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"ContextualPrecision.Metric()"})}),(0,r.jsx)(t.td,{children:"Initializes a new instance"})]})})]}),"\n",(0,r.jsx)(t.p,{children:"Here's an example of how to use Contextual Precision:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-csharp",children:"var metric = ContextualPrecision.Metric();\r\nvar result = metric.Evaluate(modelOutput);\n"})}),"\n",(0,r.jsx)(t.h3,{id:"optional-parameters",children:"Optional Parameters"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"threshold"})}),(0,r.jsx)(t.td,{children:"A float representing the minimum passing threshold, defaulting to 0.5."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"include_reason"})}),(0,r.jsxs)(t.td,{children:["A boolean that, when set to ",(0,r.jsx)(t.code,{children:"True"}),", provides a reason for its evaluation score. Default is ",(0,r.jsx)(t.code,{children:"True"}),"."]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"strict_mode"})}),(0,r.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect precision, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,r.jsx)(t.code,{children:"False"}),"."]})]})]})]}),"\n",(0,r.jsx)(t.h2,{id:"samples",children:"Samples"}),"\n",(0,r.jsx)(t.p,{children:"We've given you a sample to work with that evaluates bla bla bla. Here's how you can run these samples:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-csharp",children:"var metric = ContextualPrecision.Metric();\r\nvar result = metric.Evaluate(modelOutput);\n"})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);