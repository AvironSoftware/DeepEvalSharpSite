"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[323],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var r=t(6540);const s={},l=r.createContext(s);function a(e){const n=r.useContext(l);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(l.Provider,{value:n},e.children)}},9926:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"test-cases","title":"Test Cases","description":"Quick Summary","source":"@site/docs/test-cases.md","sourceDirName":".","slug":"/test-cases","permalink":"/docs/test-cases","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/docs/introduction"},"next":{"title":"Datasets","permalink":"/docs/datasets"}}');var s=t(4848),l=t(8453);const a={},i="Test Cases",c={},o=[{value:"Quick Summary",id:"quick-summary",level:2},{value:"Parameters",id:"parameters",level:2},{value:"InitialInput",id:"initialinput",level:3},{value:"ExpectedOutput",id:"expectedoutput",level:3},{value:"ActualOutput",id:"actualoutput",level:3},{value:"Context",id:"context",level:3},{value:"RetrievalContext",id:"retrievalcontext",level:3},{value:"ToolsCalled",id:"toolscalled",level:3},{value:"ExpectedTools",id:"expectedtools",level:3},{value:"Context vs RetrievalContext",id:"context-vs-retrievalcontext",level:2},{value:"Usage Example",id:"usage-example",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"test-cases",children:"Test Cases"})}),"\n",(0,s.jsx)(n.h2,{id:"quick-summary",children:"Quick Summary"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"EvaluatorTestData"})," is the core data structure in EvalSharp to define individual evaluation items in .NET. All fields are optional\u2014you only include what your metrics need."]}),"\n",(0,s.jsxs)(n.p,{children:["An ",(0,s.jsx)(n.code,{children:"EvaluatorTestData"})," has ",(0,s.jsx)(n.strong,{children:"seven"})," parameters:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"InitialInput"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ExpectedOutput"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ActualOutput"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"Context"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"RetrievalContext"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ToolsCalled"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ExpectedTools"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,s.jsx)(n.h3,{id:"initialinput",children:"InitialInput"}),"\n",(0,s.jsx)(n.p,{children:"The text prompt or user input sent to the LLM."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," Required by metrics that evaluate input formatting or conversational flow."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'var data = new EvaluatorTestData {\r\n    InitialInput = "What is the capital of France?"\r\n};\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"expectedoutput",children:"ExpectedOutput"}),"\n",(0,s.jsx)(n.p,{children:"The ideal response you want the LLM to produce."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," Used by output-comparison metrics (e.g., equality, similarity)."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'var data = new EvaluatorTestData {\r\n    ExpectedOutput = "Paris"\r\n};\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"actualoutput",children:"ActualOutput"}),"\n",(0,s.jsx)(n.p,{children:"The actual text returned by the LLM during evaluation."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," Compare against ",(0,s.jsx)(n.code,{children:"ExpectedOutput"})," or inspect generation quality."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"var data = new EvaluatorTestData {\r\n    ActualOutput = llm.Generate(prompt)\r\n};\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"context",children:"Context"}),"\n",(0,s.jsx)(n.p,{children:"Curated, correct background facts you want the LLM to reference (gold standard)."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"List<string>?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," RAG and grounding metrics compare generated text against this reference."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'var data = new EvaluatorTestData {\r\n    Context = new List<string> { "Paris is the capital of France." }\r\n};\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"retrievalcontext",children:"RetrievalContext"}),"\n",(0,s.jsx)(n.p,{children:"The data your application actually fetched at runtime (e.g., from a search or vector database)."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"List<string>?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," Evaluate retriever performance by comparing ",(0,s.jsx)(n.code,{children:"RetrievalContext"})," against ",(0,s.jsx)(n.code,{children:"Context"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'var data = new EvaluatorTestData {\r\n    RetrievalContext = new List<string> { "Capital cities: Paris, Berlin, Madrid" }\r\n};\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"toolscalled",children:"ToolsCalled"}),"\n",(0,s.jsx)(n.p,{children:"Tools the LLM invoked during execution."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"List<ToolCall>?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," Agentic metrics verify which tools were used (e.g., API calls, calculators)."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'var data = new EvaluatorTestData {\r\n    ToolsCalled = new List<ToolCall> {\r\n        new ToolCall { Name = "WebSearch" }\r\n    }\r\n};\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"expectedtools",children:"ExpectedTools"}),"\n",(0,s.jsx)(n.p,{children:"The tools you intended the LLM to call for optimal performance."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"List<ToolCall>?"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage:"})," Compare against ",(0,s.jsx)(n.code,{children:"ToolsCalled"})," in tool-correctness metrics."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'var data = new EvaluatorTestData {\r\n    ExpectedTools = new List<ToolCall> {\r\n        new ToolCall { Name = "WebSearch" }\r\n    }\r\n};\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"context-vs-retrievalcontext",children:"Context vs RetrievalContext"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context"})," is the textbook: the perfect facts you provide."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RetrievalContext"})," is the student\u2019s notes: the real data fetched at runtime."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This distinction helps you pinpoint whether errors arise from retrieval or generation."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"usage-example",children:"Usage Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'using EvalSharp.Scoring;\r\n\r\n// Define test data with only needed fields\r\nvar testData = new EvaluatorTestData\r\n{\r\n    InitialInput    = "What\u2019s the tallest mountain?",\r\n    ExpectedOutput  = "Mount Everest",\r\n    Context          = new List<string> { "Mount Everest is 29,032 ft." },\r\n    RetrievalContext = new List<string> { "Mount Everest 29032 ft" }\r\n};\r\n\r\n// Run evaluation\r\nvar config = new ContextualPrecisionMetricConfiguration\r\n{\r\n    IncludeReason = true\r\n};\r\nvar contextualPrecMetric = new ContextualPrecisionMetric(ChatClient.GetInstance(), config);\r\nvar score = await contextualPrecMetric.ScoreAsync(context);\r\nConsole.WriteLine($"Score: {result.Score}, Passed: {result.Result}");\n'})})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);