"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[458],{136:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Quick Start","href":"/DeepEvalSharpSite/docs/quick-start","docId":"quick-start","unlisted":false},{"type":"link","label":"Introduction","href":"/DeepEvalSharpSite/docs/introduction","docId":"introduction","unlisted":false},{"type":"link","label":"Test Cases","href":"/DeepEvalSharpSite/docs/test-cases","docId":"test-cases","unlisted":false},{"type":"link","label":"Datasets","href":"/DeepEvalSharpSite/docs/datasets","docId":"datasets","unlisted":false},{"type":"category","label":"Metrics","items":[{"type":"link","label":"Answer Relevancy Metric","href":"/DeepEvalSharpSite/docs/metrics/AnswerRelevancyMetric","docId":"metrics/AnswerRelevancyMetric","unlisted":false},{"type":"link","label":"Bias Metric","href":"/DeepEvalSharpSite/docs/metrics/BiasMetric","docId":"metrics/BiasMetric","unlisted":false},{"type":"link","label":"Contextual Precision Metric","href":"/DeepEvalSharpSite/docs/metrics/ContextualPrecisionMetric","docId":"metrics/ContextualPrecisionMetric","unlisted":false},{"type":"link","label":"Contextual Recall Metric","href":"/DeepEvalSharpSite/docs/metrics/ContextualRecallMetric","docId":"metrics/ContextualRecallMetric","unlisted":false},{"type":"link","label":"Custom Metric","href":"/DeepEvalSharpSite/docs/metrics/CustomMetric","docId":"metrics/CustomMetric","unlisted":false},{"type":"link","label":"Faithfulness Metric","href":"/DeepEvalSharpSite/docs/metrics/FaithfulnessMetric","docId":"metrics/FaithfulnessMetric","unlisted":false},{"type":"link","label":"GEval Metric","href":"/DeepEvalSharpSite/docs/metrics/GEvalMetric","docId":"metrics/GEvalMetric","unlisted":false},{"type":"link","label":"Hallucination Metric","href":"/DeepEvalSharpSite/docs/metrics/HallucinationMetric","docId":"metrics/HallucinationMetric","unlisted":false},{"type":"link","label":"Match Metric","href":"/DeepEvalSharpSite/docs/metrics/MatchMetric","docId":"metrics/MatchMetric","unlisted":false},{"type":"link","label":"Prompt Alignment Metric","href":"/DeepEvalSharpSite/docs/metrics/PromptAlignmentMetric","docId":"metrics/PromptAlignmentMetric","unlisted":false},{"type":"link","label":"Summarization Metric","href":"/DeepEvalSharpSite/docs/metrics/SummarizationMetric","docId":"metrics/SummarizationMetric","unlisted":false},{"type":"link","label":"Task Completion Metric","href":"/DeepEvalSharpSite/docs/metrics/TaskCompletionMetric","docId":"metrics/TaskCompletionMetric","unlisted":false},{"type":"link","label":"Tool Correctness Metric","href":"/DeepEvalSharpSite/docs/metrics/ToolCorrectnessMetric","docId":"metrics/ToolCorrectnessMetric","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"datasets":{"id":"datasets","title":"Datasets","description":"Loading Data into Evaluator","sidebar":"tutorialSidebar"},"introduction":{"id":"introduction","title":"DeepEvalSharp","description":"DeepEvalSharp is a powerful and extensible suite of LLM evaluation metrics built for the .NET ecosystem. Whether you\'re evaluating an intelligent chatbot, summarization tool, or agent-based workflow, DeepEvalSharp gives you the tools to measure LLM performance with precision and transparency.","sidebar":"tutorialSidebar"},"metrics/AnswerRelevancyMetric":{"id":"metrics/AnswerRelevancyMetric","title":"Answer Relevancy Metric","description":"The Answer Relevancy Metric evaluates how relevant an LLM-generated ActualOutput is in relation to the given InitialInput. This metric is particularly useful for assessing Retrieval-Augmented Generation (RAG) pipelines and ensuring that responses remain on-topic and directly address the input query. The Answer Relevancy Metric provides a reason for its evaluation score, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/BiasMetric":{"id":"metrics/BiasMetric","title":"Bias Metric","description":"The Bias Metric evaluates whether an LLM-generated ActualOutput contains gender, racial, political, or geographical bias, using an LLM-as-a-judge, referenceless approach to safety and fairness evaluation. It flags biased opinions and provides a reason for its score, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/ContextualPrecisionMetric":{"id":"metrics/ContextualPrecisionMetric","title":"Contextual Precision Metric","description":"The Contextual Precision Metric evaluates how well a Retrieval-Augmented Generation (RAG) pipeline\'s retriever ranks relevant context higher than irrelevant context for a given input. This metric helps ensure that an LLM receives the most useful information, improving the accuracy and quality of generated responses. The Contextual Precision Metric provides an explanation for its evaluation score, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/ContextualRecallMetric":{"id":"metrics/ContextualRecallMetric","title":"Contextual Recall Metric","description":"The Contextual Recall Metric evaluates the effectiveness of your Retrieval-Augmented Generation (RAG) pipeline\'s retriever by assessing how well the retrieved context (RetrievalContext) aligns with the expected output (ExpectedOutput). This metric ensures that the retriever captures and provides all relevant information necessary for generating accurate responses. Additionally, it offers explanations for its scores, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/CustomMetric":{"id":"metrics/CustomMetric","title":"Custom Metric","description":"The Custom Metric feature allows you to define your own evaluation logic, either by composing existing DeepEvalSharp metrics or by creating new rules tailored to your application. These user-defined metrics enable bespoke model validation for use cases not covered by out-of-the-box tools.","sidebar":"tutorialSidebar"},"metrics/FaithfulnessMetric":{"id":"metrics/FaithfulnessMetric","title":"Faithfulness Metric","description":"The Faithfulness Metric assesses the quality of your Retrieval-Augmented Generation (RAG) pipeline\'s generator by evaluating whether the ActualOutput factually aligns with the contents of your RetrievalContext. This metric focuses on identifying contradictions between the generated output and the provided context, ensuring that the information presented is accurate and trustworthy. Additionally, it offers explanations for its scores, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/GEvalMetric":{"id":"metrics/GEvalMetric","title":"GEval Metric","description":"G-Eval is a framework within NEval that leverages large language models (LLMs) with chain-of-thought (CoT) prompting to assess LLM outputs based on customizable criteria. This versatile metric allows for human-like evaluations across various use cases by defining specific evaluation criteria or steps. Users can create custom metrics by specifying parameters such as \'input\' and \'actualoutput\', and optionally \'expectedoutput\' and \'context\', tailoring the evaluation to their specific needs. G-Eval also offers flexibility in configuration, including options for setting evaluation steps, thresholds, and selecting different LLM models.","sidebar":"tutorialSidebar"},"metrics/HallucinationMetric":{"id":"metrics/HallucinationMetric","title":"Hallucination Metric","description":"The Hallucination Metric evaluates whether an LLM-generated ActualOutput contains fabricated or unsupported information by comparing it against the provided Context, using an LLM-as-a-judge, reference-based approach. It flags hallucinated content and provides a reason for its score, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/MatchMetric":{"id":"metrics/MatchMetric","title":"Match Metric","description":"The Match Metric evaluates whether an LLM-generated ActualOutput correctly matches the ExpectedOutput according to configurable matching rules. This metric provides flexible options for exact matching, regex-based matching, or matching output that follows the occurrence of a specific string.","sidebar":"tutorialSidebar"},"metrics/PromptAlignmentMetric":{"id":"metrics/PromptAlignmentMetric","title":"Prompt Alignment Metric","description":"The Prompt Alignment Metric evaluates whether an LLM-generated ActualOutput aligns with the instructions specified in your prompt template, using an LLM-as-a-judge, referenceless approach. It flags deviations and provides a reason for its score, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/SummarizationMetric":{"id":"metrics/SummarizationMetric","title":"Summarization Metric","description":"The Summarization Metric leverages an LLM as a judge to assess if your model\u2019s ActualOutput is both factually accurate and sufficiently comprehensive when compared to the original InitialInput. It generates two sub-scores\u2014a contradiction-and-fabrication\u2013detecting alignment score and a key-information\u2013measuring coverage score\u2014takes the lower of the two as the final result, and supplies a human-readable explanation for its judgment, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"metrics/TaskCompletionMetric":{"id":"metrics/TaskCompletionMetric","title":"Task Completion Metric","description":"The Task Completion Metric employs an LLM as a judge to measure how well an agent carries out the task specified in its InitialInput, taking into account both the ToolsCalled and the agent\u2019s ActualOutput. The Task Completion Metric is an agentic, referenceless LLM-as-a-judge metric that measures how well an LLM agent accomplishes a user-specified task.","sidebar":"tutorialSidebar"},"metrics/ToolCorrectnessMetric":{"id":"metrics/ToolCorrectnessMetric","title":"Tool Correctness Metric","description":"The Tool Correctness Metric measures how accurately an LLM agent invokes its tools by comparing the actual calls made (ToolsCalled) against the list of expected tools (ExpectedTools). It supports configurable strictness\u2014by default it checks just the tool names, but you can require matching input parameters and outputs for full verification\u2014and returns both a numeric score and a human-readable explanation of any mismatches, making it a self-explaining LLM-Eval tool.","sidebar":"tutorialSidebar"},"quick-start":{"id":"quick-start","title":"Quick Start","description":"Follow these steps to get up and running with DeepEvalSharp in minutes.","sidebar":"tutorialSidebar"},"test-cases":{"id":"test-cases","title":"Test Cases","description":"Quick Summary","sidebar":"tutorialSidebar"}}}}')}}]);