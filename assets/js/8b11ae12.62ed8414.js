"use strict";(self.webpackChunkdocs_src=self.webpackChunkdocs_src||[]).push([[664],{3043:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"metrics/TaskCompletionMetric","title":"Task Completion Metric","description":"The Task Completion Metric employs an LLM as a judge to measure how well an agent carries out the task specified in its InitialInput, taking into account both the ToolsCalled and the agent\u2019s ActualOutput. The Task Completion Metric is an agentic, referenceless LLM-as-a-judge metric that measures how well an LLM agent accomplishes a user-specified task.","source":"@site/docs/metrics/TaskCompletionMetric.md","sourceDirName":"metrics","slug":"/metrics/TaskCompletionMetric","permalink":"/DeepEvalSharpSite/docs/metrics/TaskCompletionMetric","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Summarization Metric","permalink":"/DeepEvalSharpSite/docs/metrics/SummarizationMetric"},"next":{"title":"Tool Correctness Metric","permalink":"/DeepEvalSharpSite/docs/metrics/ToolCorrectnessMetric"}}');var r=s(4848),i=s(8453);const o={},a="Task Completion Metric",l={},c=[{value:"When you should use Task Completion Metric",id:"when-you-should-use-task-completion-metric",level:4},{value:"When you SHOULDN&#39;T use Task Completion Metric",id:"when-you-shouldnt-use-task-completion-metric",level:4},{value:"How to use",id:"how-to-use",level:2},{value:"Required Data Fields",id:"required-data-fields",level:3},{value:"Optional Configuration Parameters",id:"optional-configuration-parameters",level:3}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"task-completion-metric",children:"Task Completion Metric"})}),"\n",(0,r.jsxs)(t.p,{children:["The Task Completion Metric employs an LLM as a judge to measure how well an agent carries out the task specified in its ",(0,r.jsx)(t.code,{children:"InitialInput"}),", taking into account both the ",(0,r.jsx)(t.code,{children:"ToolsCalled"})," and the agent\u2019s ",(0,r.jsx)(t.code,{children:"ActualOutput"}),". The Task Completion Metric is an agentic, referenceless LLM-as-a-judge metric that measures how well an LLM agent accomplishes a user-specified task."]}),"\n",(0,r.jsx)(t.h4,{id:"when-you-should-use-task-completion-metric",children:"When you should use Task Completion Metric"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Assessing Agent Effectiveness"})," \u2013 Verify that your LLM agent successfully completes user-defined tasks by invoking necessary tools and providing correct outputs."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Benchmarking Agent Configurations"})," \u2013 Compare different agent strategies, tool sets, or LLM models on their task completion performance."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Debugging Agent Workflows"})," \u2013 Identify under- or over-utilization of tools within your agent pipeline to improve tool integration."]}),"\n"]}),"\n",(0,r.jsx)(t.h4,{id:"when-you-shouldnt-use-task-completion-metric",children:"When you SHOULDN'T use Task Completion Metric"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Non-Agentic Outputs"})," \u2013 This metric is not applicable if your LLM outputs static text without tool calls."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Factual Accuracy Checks"})," \u2013 For pure factual verification, consider Faithfulness or Hallucination metrics instead."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"High-Throughput Requirements"})," \u2013 LLM-as-a-judge evaluations incur API calls and may not suit pipelines where speed and scale are primary concerns."]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"how-to-use",children:"How to use"}),"\n",(0,r.jsxs)(t.p,{children:["The Task Completion Metric requires ",(0,r.jsx)(t.code,{children:"InitialInput"}),", ",(0,r.jsx)(t.code,{children:"ActualOutput"}),", and ",(0,r.jsx)(t.code,{children:"ToolsCalled"})," to function. You can instantiate an Task Completion metric with optional parameters to customize its behavior."]}),"\n",(0,r.jsx)(t.p,{children:"Add Task Completion Metric to your evaluator:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Method"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"AddTaskCompletion(bool includeReason = true, bool strictMode = false, double threshold = 0.5)"})}),(0,r.jsx)(t.td,{children:"Creates the Task Completion metric and adds it to the evaluator."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"AddTaskCompletion(TaskCompletionMetricConfiguration config)"})}),(0,r.jsx)(t.td,{children:"Creates the Task Completion metric and adds it to the evaluator."})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"Here's an example of how to use Task Completion metric:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-csharp",children:'// 1) Prepare your data\r\nvar cases = new[]\r\n{\r\n    new TType\r\n    {\r\n        UserInput    = "I need to reset my password.",\r\n        LLMOutput   = "I\u2019ve sent a password reset link to your registered email.",\r\n        ToolsCalled = [\r\n            new ToolCall\r\n            {\r\n                Name = "Reset Password",\r\n                Description = "Resets a user\'s password and sends a reset link.",\r\n                InputParameters = new Dictionary<string, object?> { { "user_request", "reset password" } },\r\n                Output = "Password reset link sent."\r\n            }\r\n        ]\r\n    }\r\n};\r\n\r\n// 2) Create evaluator, mapping your case \u2192 EvaluatorTestData\r\nvar evaluator = Evaluator.FromData(\r\n    ChatClient.GetInstance(),\r\n    cases,\r\n    c => new EvaluatorTestData\r\n    {\r\n        InitialInput    = c.UserInput,\r\n        ActualOutput    = c.LLMOutput,\r\n        ToolsCalled     = c.ToolsCalled\r\n    }\r\n);\r\n\r\n// 3) Add metric and run\r\nevaluator.AddTaskCompletion(includeReason: true);\r\nvar result = await evaluator.RunAsync();\n'})}),"\n",(0,r.jsx)(t.h3,{id:"required-data-fields",children:"Required Data Fields"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"InitialInput"})}),(0,r.jsx)(t.td,{children:"A string That represents the initial input is the user interaction with the LLM."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"ActualOutput"})}),(0,r.jsx)(t.td,{children:"A string That represents the actual output of the test case from the LLM."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"ToolsCalled"})}),(0,r.jsxs)(t.td,{children:["A list of ",(0,r.jsx)(t.code,{children:"DeepEvalSharp.Models.ToolCall"}),"'s which are tools your LLM actually invoked during execution."]})]})]})]}),"\n",(0,r.jsx)(t.h3,{id:"optional-configuration-parameters",children:"Optional Configuration Parameters"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"Threshold"})}),(0,r.jsx)(t.td,{children:"A float representing the minimum passing score, defaulting to 0.5."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"IncludeReason"})}),(0,r.jsxs)(t.td,{children:["A boolean that, when set to ",(0,r.jsx)(t.code,{children:"True"}),", provides a reason for the metric score. Default is ",(0,r.jsx)(t.code,{children:"True"}),"."]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"StrictMode"})}),(0,r.jsxs)(t.td,{children:["Enforces a binary metric score\u20141 for perfect relevance, 0 otherwise\u2014setting the threshold to 1. Default is ",(0,r.jsx)(t.code,{children:"False"}),"."]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,t,s)=>{s.d(t,{R:()=>o,x:()=>a});var n=s(6540);const r={},i=n.createContext(r);function o(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);